---
layout: layouts/tutorial.html
title: Complete Project Walkthrough | 14.33
current_page: project-walkthrough
---
<h1>Complete Project Walkthrough</h1>
<p class="subtitle">From raw data to publication-ready output — start to finish</p>

<p>This walkthrough takes you through an entire empirical project: loading data, cleaning, merging multiple datasets, running a difference-in-differences analysis with event study, and producing publication-quality tables and figures. We also cover instrumental variables and regression discontinuity as standalone analyses.</p>

<!-- ============================================================ -->
<!-- PROJECT DIRECTORY SETUP -->
<!-- ============================================================ -->
<section id="project-setup" class="tutorial-section">
  <h2>Project Setup</h2>

  <p>An empirical project separates <strong>raw data</strong> from <strong>processed data</strong>, and <strong>build code</strong> (data processing) from <strong>analysis code</strong> (regressions, tables, figures). The download below contains the full project with this structure already in place.</p>

  {% capture callout_content %}<div style="text-align: center;">
    <p>Each ZIP contains all six datasets, the full directory structure shown below, and code in your chosen language. Pick one and unzip — you're ready to go.</p>
    <div style="display: flex; gap: 12px; justify-content: center; flex-wrap: wrap;">
      <a href="data/pkg-stata.zip" download class="download-btn">Stata</a>
      <a href="data/pkg-r.zip" download class="download-btn">R</a>
      <a href="data/pkg-python.zip" download class="download-btn">Python</a>
    </div>
  </div>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "Download the Project", content: callout_content %}

  <h3>What's Inside</h3>

  <p>Unzipping gives you a single project folder (e.g. <code>pkg-stata/</code>) with this structure:</p>

  {% capture sc %}pkg-stata/
├── master.do                  ← Runs everything in order
├── build/
│   ├── code/
│   │   ├── 01_collapse_crashes.do
│   │   └── 02_merge_datasets.do
│   ├── input/                 ← Raw data (NEVER modified by code)
│   │   ├── crash_data.csv
│   │   ├── state_demographics.dta
│   │   ├── policy_adoptions.csv
│   │   └── state_names.csv
│   └── output/                ← Processed data (created by build scripts)
│       ├── crashes_state_year.dta
│       └── analysis_panel.dta
└── analysis/
    ├── code/
    │   ├── 01_descriptive_table.do
    │   ├── 02_dd_regression.do
    │   ├── 03_event_study.do
    │   ├── 04_iv.do
    │   └── 05_rd.do
    └── output/
        ├── tables/            ← Regression tables (.tex)
        │   ├── descriptive_table.tex
        │   ├── dd_results.tex
        │   ├── iv_results.tex
        │   └── rd_results.tex
        └── figures/           ← Plots and graphs (.png, .pdf)
            └── event_study.png{% endcapture %}
  {% include "code-block.liquid", stata: sc, stata_label: "Directory Structure" %}

  <h3>Key Principles</h3>
  <ul>
    <li><strong>Raw data is read-only.</strong> Files in <code>build/input/</code> are never modified by your code. If you need to clean or transform data, save the result to <code>build/output/</code>.</li>
    <li><strong>Build vs. analysis.</strong> Build scripts (<code>build/code/</code>) turn raw data into analysis-ready datasets. Analysis scripts (<code>analysis/code/</code>) take those datasets and produce results. This separation means you can change a regression without re-processing all your data.</li>
    <li><strong>Numbered scripts.</strong> Prefix scripts with <code>01_</code>, <code>02_</code>, etc. so the execution order is obvious. A <code>master.do</code> file at the root runs them all sequentially.</li>
    <li><strong>Outputs are reproducible.</strong> Everything in <code>build/output/</code> and <code>analysis/output/</code> can be regenerated by running <code>master.do</code>. Never hand-edit output files.</li>
  </ul>

  <h3>The Master File</h3>
  <p>The <code>master.do</code> sets global paths and runs every script in order. Anyone should be able to replicate your results by changing one path and running this file.</p>

  {% capture sc %}* MASTER DO FILE
clear all
set more off

* Set root directory (CHANGE THIS to your project path)
global root "."
global build "$root/build"
global analysis "$root/analysis"

cd "$root"

* Build stage
do "$build/code/01_collapse_crashes.do"
do "$build/code/02_merge_datasets.do"

* Analysis stage
do "$analysis/code/01_descriptive_table.do"
do "$analysis/code/02_dd_regression.do"
do "$analysis/code/03_event_study.do"
do "$analysis/code/04_iv.do"
do "$analysis/code/05_rd.do"{% endcapture %}
  {% capture rc %}# master.R
rm(list = ls())

# Set root directory (CHANGE THIS to your project path)
root <- "."
build <- file.path(root, "build")
analysis <- file.path(root, "analysis")

# Build stage
source(file.path(build, "code", "01_collapse_crashes.R"))
source(file.path(build, "code", "02_merge_datasets.R"))

# Analysis stage
source(file.path(analysis, "code", "01_descriptive_table.R"))
source(file.path(analysis, "code", "02_dd_regression.R"))
source(file.path(analysis, "code", "03_event_study.R"))
source(file.path(analysis, "code", "04_iv.R"))
source(file.path(analysis, "code", "05_rd.R")){% endcapture %}
  {% capture pc %}# master.py
import subprocess, sys, os

root = "."
build = os.path.join(root, "build")
analysis = os.path.join(root, "analysis")

scripts = [
    f"{build}/code/01_collapse_crashes.py",
    f"{build}/code/02_merge_datasets.py",
    f"{analysis}/code/01_descriptive_table.py",
    f"{analysis}/code/02_dd_regression.py",
    f"{analysis}/code/03_event_study.py",
    f"{analysis}/code/04_iv.py",
    f"{analysis}/code/05_rd.py",
]

for script in scripts:
    print(f"Running {script}...")
    subprocess.run([sys.executable, script], check=True){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}
</section>

<!-- ============================================================ -->
<!-- PART A: READING DATA -->
<!-- ============================================================ -->
<section id="reading-data" class="tutorial-section">
  <h2>Part A: Reading Data</h2>

  <p>Our project uses data from multiple sources in different formats. Start by reading each one in.</p>

  <h3>Reading a CSV File</h3>
  <p>The crash data has one row per crash. This is the raw, granular data we'll need to aggregate later.</p>

  {% capture sc %}* Read crash-level data (CSV)
import delimited "$build/input/crash_data.csv", clear
describe
summarize
tab severity{% endcapture %}
  {% capture rc %}pacman::p_load(data.table)

# Read crash-level data (CSV)
crashes <- fread("build/input/crash_data.csv")
str(crashes)
summary(crashes)
table(crashes$severity){% endcapture %}
  {% capture pc %}import pandas as pd

# Read crash-level data (CSV)
crashes = pd.read_csv("build/input/crash_data.csv")
print(crashes.info())
print(crashes.describe())
print(crashes['severity'].value_counts()){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <h3>Reading a Stata (.dta) File</h3>
  <p>The demographics data is in Stata's native format. All three languages can read <code>.dta</code> files.</p>

  {% capture sc %}* Read Stata file
use "$build/input/state_demographics.dta", clear
describe
summarize population median_income pct_urban{% endcapture %}
  {% capture rc %}pacman::p_load(haven)

# Read Stata file
demographics <- read_dta("build/input/state_demographics.dta")
str(demographics)
summary(demographics[, c("population", "median_income", "pct_urban")]){% endcapture %}
  {% capture pc %}import pandas as pd

# Read Stata file
demographics = pd.read_stata("build/input/state_demographics.dta")
print(demographics.info())
print(demographics[['population', 'median_income', 'pct_urban']].describe()){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}
</section>

<!-- ============================================================ -->
<!-- PART B: COLLAPSING DATA -->
<!-- ============================================================ -->
<section id="collapsing" class="tutorial-section">
  <h2>Part B: Filtering, Collapsing, Reshaping, and Saving Intermediate Files</h2>

  <p>The crash data has one row per crash, but our analysis is at the <strong>state-year</strong> level. We need to: (1) <strong>filter</strong> the data to keep only the crashes we care about, (2) <strong>collapse</strong> to count crashes by state, year, and severity, (3) <strong>reshape</strong> from long to wide so each severity type becomes its own column, and (4) <strong>save</strong> the intermediate file.</p>

  {% capture sc %}* Load crash data
import delimited "$build/input/crash_data.csv", clear

* --- FILTER: Keep only fatal and serious crashes ---
drop if severity == "minor"

* --- COLLAPSE: Count crashes by state-year-severity (long format) ---
gen one = 1
collapse (sum) n_crashes = one, by(state_fips year severity)

* --- RESHAPE: Wide so each severity type becomes its own column ---
reshape wide n_crashes, i(state_fips year) j(severity) string

* Rename for clarity
rename n_crashesfatal fatal_crashes
rename n_crashesserious serious_crashes

* Compute total and fatal share
gen total_crashes = fatal_crashes + serious_crashes
gen fatal_share = fatal_crashes / total_crashes

* Save intermediate file
save "$build/output/crashes_state_year.dta", replace
describe{% endcapture %}
  {% capture rc %}pacman::p_load(data.table)

# Load crash data
crashes <- fread("build/input/crash_data.csv")

# Filter: keep only fatal and serious crashes
crashes <- crashes[severity != "minor"]

# Collapse to state-year-severity level (long format)
crashes_long <- crashes[, .(n_crashes = .N), by = .(state_fips, year, severity)]

# Reshape from long to wide: one column per severity type
crashes_sy <- dcast(crashes_long, state_fips + year ~ severity,
                    value.var = "n_crashes", fill = 0L)

# Rename and compute totals
setnames(crashes_sy, c("fatal", "serious"), c("fatal_crashes", "serious_crashes"))
crashes_sy[, total_crashes := fatal_crashes + serious_crashes]
crashes_sy[, fatal_share := fatal_crashes / total_crashes]

# Save intermediate file
fwrite(crashes_sy, "build/output/crashes_state_year.csv")
str(crashes_sy){% endcapture %}
  {% capture pc %}import pandas as pd

# Load crash data
crashes = pd.read_csv("build/input/crash_data.csv")

# Filter: keep only fatal and serious crashes
crashes = crashes[crashes['severity'] != 'minor']

# Collapse to state-year-severity level (long format)
crashes_long = (crashes.groupby(['state_fips', 'year', 'severity'])
                .size().reset_index(name='n_crashes'))

# Reshape (pivot) from long to wide: one column per severity type
crashes_sy = crashes_long.pivot_table(
    index=['state_fips', 'year'], columns='severity',
    values='n_crashes', fill_value=0
).reset_index()
crashes_sy.columns.name = None
crashes_sy = crashes_sy.rename(columns={'fatal': 'fatal_crashes',
                                         'serious': 'serious_crashes'})

# Compute total and fatal share
crashes_sy['total_crashes'] = crashes_sy['fatal_crashes'] + crashes_sy['serious_crashes']
crashes_sy['fatal_share'] = crashes_sy['fatal_crashes'] / crashes_sy['total_crashes']

# Save intermediate file
crashes_sy.to_csv("build/output/crashes_state_year.csv", index=False)
print(crashes_sy.info()){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  {% capture callout_content %}<p>Separating your build code (data processing) from analysis code is a key principle of project organization. Save collapsed data so you can modify your analysis without re-running expensive data processing steps every time.</p>{% endcapture %}
  {% include "callout.liquid", type: "success", heading: "Why Save Intermediate Files?", content: callout_content %}
</section>

<!-- ============================================================ -->
<!-- PART C: MERGING 3+ DATASETS -->
<!-- ============================================================ -->
<section id="merging" class="tutorial-section">
  <h2>Part C: Merging Multiple Datasets</h2>

  <p>Now we merge four datasets together: (1) aggregated crashes, (2) state demographics, (3) policy adoption dates, and (4) state names. Each merge uses <code>state_fips</code> as the key, with some using <code>year</code> as well.</p>

  {% capture sc %}* Convert CSVs to .dta for merging
import delimited "$build/input/policy_adoptions.csv", clear
save "$build/output/policy_adoptions.dta", replace

import delimited "$build/input/state_names.csv", clear
save "$build/output/state_names.dta", replace

* Load the main dataset and merge everything sequentially
use "$build/output/crashes_state_year.dta", clear

* Merge 1: Demographics — keep only matched rows
merge 1:1 state_fips year using "$build/input/state_demographics.dta", ///
    keep(match) nogen

* Merge 2: Policy adoptions — keep master + matched (some states never adopt)
merge m:1 state_fips using "$build/output/policy_adoptions.dta", ///
    keep(master match) nogen

* Merge 3: State names — keep only matched rows
merge m:1 state_fips using "$build/output/state_names.dta", ///
    keep(match) nogen

* Create treatment indicator
gen treated = (year >= adoption_year & !missing(adoption_year))
gen log_pop = ln(population)

* Save final analysis dataset
save "$build/output/analysis_panel.dta", replace
describe{% endcapture %}
  {% capture rc %}pacman::p_load(data.table, haven)

# Load all datasets
crashes_sy <- fread("build/output/crashes_state_year.csv")
demographics <- as.data.table(read_dta("build/input/state_demographics.dta"))
policies <- fread("build/input/policy_adoptions.csv")
state_names <- fread("build/input/state_names.csv")

# Merge 1: Crashes + Demographics (on state_fips + year)
panel <- merge(crashes_sy, demographics, by = c("state_fips", "year"), all = FALSE)
cat("After merge 1:", nrow(panel), "rows\n")

# Merge 2: + Policy adoptions (on state_fips only — many:1)
panel <- merge(panel, policies, by = "state_fips", all.x = TRUE)
cat("After merge 2:", nrow(panel), "rows\n")

# Merge 3: + State names (on state_fips only — many:1)
panel <- merge(panel, state_names, by = "state_fips", all.x = TRUE)
cat("After merge 3:", nrow(panel), "rows\n")

# Create treatment indicator
panel[, treated := fifelse(
  !is.na(adoption_year) & year >= adoption_year, 1L, 0L
)]
panel[, log_pop := log(population)]

# Save final analysis dataset
fwrite(panel, "build/output/analysis_panel.csv")
str(panel){% endcapture %}
  {% capture pc %}import pandas as pd
import numpy as np

# Load all datasets
crashes_sy = pd.read_csv("build/output/crashes_state_year.csv")
demographics = pd.read_stata("build/input/state_demographics.dta")
policies = pd.read_csv("build/input/policy_adoptions.csv")
state_names = pd.read_csv("build/input/state_names.csv")

# Merge 1: Crashes + Demographics (on state_fips + year)
panel = crashes_sy.merge(demographics, on=['state_fips', 'year'], how='inner')
print(f"After merge 1: {len(panel)} rows")

# Merge 2: + Policy adoptions (on state_fips only — many:1)
panel = panel.merge(policies, on='state_fips', how='left')
print(f"After merge 2: {len(panel)} rows")

# Merge 3: + State names (on state_fips only — many:1)
panel = panel.merge(state_names, on='state_fips', how='left')
print(f"After merge 3: {len(panel)} rows")

# Create treatment indicator
panel['treated'] = (
    (panel['year'] >= panel['adoption_year']) &
    panel['adoption_year'].notna()
).astype(int)
panel['log_pop'] = np.log(panel['population'])

# Save final analysis dataset
panel.to_csv("build/output/analysis_panel.csv", index=False)
print(panel.info()){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  {% capture callout_content %}<p>After every merge, verify: (1) the number of rows is what you expect, (2) there are no unexpected missing values, and (3) the merge type (1:1, m:1, 1:m) matches your data structure. In Stata, the <code>keep()</code> option controls which observations to retain: <code>keep(match)</code> keeps only matched rows, <code>keep(master match)</code> keeps all master rows even if unmatched (like a left join). The <code>nogen</code> option suppresses the <code>_merge</code> variable. In R/Python, compare row counts before and after.</p>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "Always Check Your Merges", content: callout_content %}
</section>

<!-- ============================================================ -->
<!-- PART D: DESCRIPTIVE STATISTICS TABLE -->
<!-- ============================================================ -->
<section id="descriptive-table" class="tutorial-section">
  <h2>Part D: Descriptive Statistics Table</h2>

  <p>Before running regressions, present a descriptive statistics table. A standard format in DD papers has four columns: (1) all observations, (2) treated states in the post-treatment period, (3) treated states before treatment, and (4) never-treated states. This shows how the groups compare and whether there are large pre-treatment differences.</p>

  {% capture sc %}use "$build/output/analysis_panel.dta", clear

* Create group indicator
gen group = 3 if missing(adoption_year)
replace group = 1 if !missing(adoption_year) & year >= adoption_year
replace group = 2 if !missing(adoption_year) & year < adoption_year

label define grp 1 "Treated After" 2 "Treated Before" 3 "Untreated"
label values group grp

* Encode region for factor variable (dtable requires numeric)
encode region, gen(census_region)
label variable census_region "Census Region"

* Label variables
label variable fatal_crashes "Fatal Crashes"
label variable serious_crashes "Serious Crashes"
label variable total_crashes "Total Crashes"
label variable fatal_share "Fatal Share"
label variable population "Population"
label variable median_income "Median Income"
label variable pct_urban "Pct. Urban"

* dtable produces "mean (sd)" format by default
dtable fatal_crashes serious_crashes total_crashes ///
    fatal_share population median_income pct_urban ///
    i.census_region, ///
    by(group) ///
    nformat(%14.2fc mean sd) ///
    sample(, statistics(freq) place(seplabels))

* Rename "Total" column to "All"
collect label levels group .m "All", modify
collect style header group, title(hide)

* Export LaTeX fragment
collect export "$analysis/output/tables/descriptive_table_raw.tex", tableonly replace

* Strip \begin{table} wrapper (dtable adds it; we just need the tabular)
filefilter "$analysis/output/tables/descriptive_table_raw.tex" ///
    "$analysis/output/tables/descriptive_table_s1.tex", ///
    from("\BSbegin{table}[!h]") to("") replace
filefilter "$analysis/output/tables/descriptive_table_s1.tex" ///
    "$analysis/output/tables/descriptive_table_s2.tex", ///
    from("\BScentering") to("") replace
filefilter "$analysis/output/tables/descriptive_table_s2.tex" ///
    "$analysis/output/tables/descriptive_table.tex", ///
    from("\BSend{table}") to("") replace{% endcapture %}
  {% capture rc %}pacman::p_load(data.table, modelsummary)

panel <- fread("build/output/analysis_panel.csv")

# Create group column
panel[, ever_treated := !is.na(adoption_year)]
panel[, group := fifelse(
  !ever_treated, "Untreated",
  fifelse(treated == 1, "Treated x Post", "Treated x Pre")
)]

# Variables for the table
vars <- c("fatal_crashes", "serious_crashes", "total_crashes",
          "fatal_share", "population", "median_income", "pct_urban")

# Create publication-quality table with datasummary
datasummary(
  fatal_crashes + serious_crashes + total_crashes +
    fatal_share + population + median_income + pct_urban ~
    group * (Mean + SD),
  data = panel,
  fmt = 2,
  output = "analysis/output/tables/descriptive_table.tex"
){% endcapture %}
  {% capture pc %}import pandas as pd

panel = pd.read_csv("build/output/analysis_panel.csv")

# Create group column
panel['group'] = 'Untreated'
panel.loc[panel['adoption_year'].notna() & (panel['treated'] == 1),
          'group'] = 'Treated x Post'
panel.loc[panel['adoption_year'].notna() & (panel['treated'] == 0),
          'group'] = 'Treated x Pre'

# Variables for the table
vars = ['fatal_crashes', 'serious_crashes', 'total_crashes',
        'fatal_share', 'population', 'median_income', 'pct_urban']

# Compute means and SDs by group
stats = (panel.groupby('group')[vars]
         .agg(['mean', 'std'])
         .round(2))

# Export to LaTeX
stats.T.to_latex("analysis/output/tables/descriptive_table.tex",
                 caption="Descriptive Statistics",
                 label="tab:desc"){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <p>Here's what the exported table looks like when compiled to PDF:</p>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/descriptive_table.png" alt="Descriptive statistics table">
  </figure>

  {% capture callout_content %}<ul>
      <li><strong>Pre-treatment balance:</strong> Treated Before vs. Untreated should look similar. Big differences suggest selection into treatment.</li>
      <li><strong>Post-treatment changes:</strong> Compare Treated After vs. Treated Before. This is the raw before/after for treated units.</li>
      <li><strong>Sample composition:</strong> Report observation counts for each group at the bottom of the table.</li>
    </ul>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "What to Look For", content: callout_content %}

  <h3>From Code Output to Publication-Ready Table</h3>

  <p>The code above produces a <code>.tex</code> file containing the table body. To get a publication-ready PDF, wrap it in a standalone LaTeX document and compile:</p>

  {% capture sc %}\documentclass[border=10pt]{standalone}
\usepackage{booktabs}
\begin{document}
\input{descriptive_table.tex}
\end{document}{% endcapture %}
  {% include "code-block.liquid", stata: sc, stata_label: "LaTeX wrapper" %}

  <p>Compile with <code>pdflatex standalone_table.tex</code>. See the <a href="auxiliary-latex.html">LaTeX module</a> for installation details.</p>

  {% capture callout_content %}<ul>
      <li><strong>Use booktabs rules</strong> (<code>\toprule</code>, <code>\midrule</code>, <code>\bottomrule</code>) — never vertical lines or heavy grids</li>
      <li><strong>Standard errors in parentheses</strong> directly below coefficients</li>
      <li><strong>Stars for significance</strong> with a clear legend at the bottom</li>
      <li><strong>Round consistently:</strong> 2-3 decimal places for coefficients, 3 for standard errors</li>
      <li><strong>Label variables clearly:</strong> "Log(Population)" not "log_pop"</li>
      <li><strong>Fixed effects indicators:</strong> "Yes" or blank (not "No")</li>
      <li><strong>Sample size</strong> and <strong>R-squared</strong> at the bottom of every regression table</li>
    </ul>{% endcapture %}
  {% include "callout.liquid", type: "warning", heading: "Table Formatting Checklist", content: callout_content %}

  {% capture callout_content %}<p>Formatting tables and figures is an excellent use case for AI tools like ChatGPT or Claude. These tasks leave little room for hallucination—the AI just needs to adjust spacing, labels, and LaTeX formatting. Describe what you want and iterate.</p>{% endcapture %}
  {% include "callout.liquid", type: "success", heading: "AI Tools Are Great for Table Formatting", content: callout_content %}
</section>

<!-- ============================================================ -->
<!-- PART E: DIFF-IN-DIFF AND EVENT STUDY -->
<!-- ============================================================ -->
<section id="dd-analysis" class="tutorial-section">
  <h2>Part E: Difference-in-Differences and Event Study</h2>

  <h3>The TWFE Regression</h3>
  <p>With our merged panel, we estimate the effect of the policy on fatal crashes using two-way fixed effects (state + year FE).</p>

  {% capture sc %}use "$build/output/analysis_panel.dta", clear

* Label variables for table output
label variable treated "Treated"
label variable log_pop "Log Population"
label variable median_income "Median Income"

* Main DD regression: effect of policy on fatal crashes
reghdfe fatal_crashes treated, absorb(state_fips year) vce(cluster state_fips){% endcapture %}
  {% capture rc %}pacman::p_load(fixest, data.table)

panel <- fread("build/output/analysis_panel.csv")

# Main DD regression
main <- feols(
  fatal_crashes ~ treated | state_fips + year,
  data = panel, vcov = ~state_fips
)
summary(main){% endcapture %}
  {% capture pc %}import pyfixest as pf
import pandas as pd

panel = pd.read_csv("build/output/analysis_panel.csv")

# Main DD regression
main = pf.feols(
    "fatal_crashes ~ treated | state_fips + year",
    vcov={"CRV1": "state_fips"}, data=panel
)
print(main.summary()){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <h3>Event Study</h3>
  <p>The event study shows dynamic treatment effects and lets us check the parallel trends assumption.</p>

  {% capture sc %}use "$build/output/analysis_panel.dta", clear

* Create time-to-treatment (never-treated get a far-away value)
gen time_to_treat = year - adoption_year
replace time_to_treat = -99 if missing(adoption_year)

* Create dummies: rel_m5 ... rel_m1, rel_0 ... rel_5
forvalues t = -5/5 {
    if `t' < 0 {
        local name "m`= abs(`t')'"
    }
    else {
        local name "`t'"
    }
    gen rel_`name' = (time_to_treat == `t')
}

* Bin endpoints
replace rel_m5 = (time_to_treat <= -5) & !missing(adoption_year)
replace rel_5  = (time_to_treat >= 5)  & !missing(adoption_year)

* Regression (omit t=-1)
reghdfe fatal_crashes rel_m5 rel_m4 rel_m3 rel_m2 ///
    rel_0 rel_1 rel_2 rel_3 rel_4 rel_5 log_pop, ///
    absorb(state_fips year) vce(cluster state_fips)

estimates store event_study

* Extract coefficients and SEs into matrices
matrix b = e(b)
matrix V = e(V)

preserve
clear
set obs 11
gen t = .
gen coef = .
gen se = .

local coefs "rel_m5 rel_m4 rel_m3 rel_m2 rel_0 rel_1 rel_2 rel_3 rel_4 rel_5"
local i = 1
foreach v of local coefs {
    if `i' <= 4 { replace t = `i' - 6 in `i' }
    else        { replace t = `i' - 5 in `i' }
    replace coef = b[1, `i'] in `i'
    replace se = sqrt(V[`i', `i']) in `i'
    local i = `i' + 1
}

* Add omitted period (t = -1, coefficient = 0)
replace t = -1 in 11
replace coef = 0 in 11
replace se = 0 in 11

gen ub = coef + 1.96 * se
gen lb = coef - 1.96 * se
sort t

* Shaded confidence bands (ribbons) + connected points
twoway (rarea ub lb t, color("44 95 138%20") lwidth(none)) ///
       (connected coef t, mcolor("44 95 138") lcolor("44 95 138") ///
            msize(small) msymbol(O) lwidth(medthin)), ///
    yline(0, lcolor(gs8) lwidth(thin)) ///
    xline(-0.5, lcolor(gs8) lwidth(thin) lpattern(dash)) ///
    xtitle("Years Relative to Policy Adoption", ///
           size(large) margin(t=3) color(gs5)) ///
    ytitle("") ///
    ylabel(, labsize(large) labcolor(gs5) grid glcolor(gs14) glwidth(vthin)) ///
    xlabel(-5(1)5, labsize(large) labcolor(gs5)) ///
    title("Effect on Fatal Crashes", ///
          position(11) size(large) color(gs5)) ///
    legend(off) ///
    graphregion(color(white) margin(medium)) ///
    plotregion(color(white) margin(small))

graph export "$analysis/output/figures/event_study.png", replace width(2400) height(1800)
restore{% endcapture %}
  {% capture rc %}pacman::p_load(fixest, ggfixest, data.table)

panel <- fread("build/output/analysis_panel.csv")

# Create time-to-treatment
# Never-treated: set to -1000
panel[, time_to_treat := fifelse(
  is.na(adoption_year), -1000L,
  as.integer(year - adoption_year)
)]
panel[, ever_treated := fifelse(!is.na(adoption_year), 1L, 0L)]

# Bin endpoints
panel[, time_to_treat := fifelse(
  time_to_treat == -1000, -1000L,
  as.integer(pmax(-5, pmin(5, time_to_treat)))
)]

# Event study regression
es <- feols(
  fatal_crashes ~ i(time_to_treat, ever_treated,
                     ref = c(-1, -1000)) + log_pop
    | state_fips + year,
  data = panel, vcov = ~state_fips
)
summary(es)

# Plot
ggiplot(es) +
  labs(x = "Years Relative to Policy Adoption",
       y = "Effect on Fatal Crashes") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_minimal()
ggsave("analysis/output/figures/event_study.png", width = 8, height = 5, dpi = 300){% endcapture %}
  {% capture pc %}import pyfixest as pf
import pandas as pd
import numpy as np

panel = pd.read_csv("build/output/analysis_panel.csv")

# Create time-to-treatment
panel['time_to_treat'] = panel['year'] - panel['adoption_year']
panel['time_to_treat'] = panel['time_to_treat'].fillna(-1000).astype(int)
panel['ever_treated'] = panel['adoption_year'].notna().astype(int)

# Bin endpoints
panel['time_to_treat'] = np.where(
    panel['time_to_treat'] == -1000, -1000,
    np.clip(panel['time_to_treat'], -5, 5)
)

# Event study regression
es = pf.feols(
    "fatal_crashes ~ i(time_to_treat, ever_treated, ref=c(-1, -1000))"
    " + log_pop | state_fips + year",
    vcov={"CRV1": "state_fips"}, data=panel
)
print(es.summary())

# Plot
es.iplot(){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <p>Here's the event study plot produced by the Stata code above:</p>
  <figure class="output-figure narrow">
    <img src="data/project-walkthrough/figures/event_study.png" alt="Event study plot">
    <figcaption>Event study showing dynamic treatment effects. Pre-treatment coefficients near zero support the parallel trends assumption.</figcaption>
  </figure>

  <h3>Making Figures Publication-Ready</h3>

  <p>Stata's default graphs look terrible. With a few tweaks, you can produce figures that look like they belong in a top journal. The key reference is <strong>Schwabish (2014)</strong>, <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.1.209" target="_blank">"An Economist's Guide to Visualizing Data"</a> (<em>JEP</em>). Tal Gross has an excellent thread showing exactly how to transform Stata's defaults into clean, publication-quality output:</p>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfmixrc23" data-bluesky-cid="bafyreihlukkgjwe52lynfbe7omkryyu7zzuqntc3ajhihingmgcl7vlhxe"><p lang="en">Make your graphs better in three easy steps…</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfmixrc23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfmj6m223" data-bluesky-cid="bafyreidjlo6y6jrr55p2loflddfepwwu2qm32xsvi53frozzvjljg3udee"><p lang="en">To start, here's a graph with the default Stata options.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfmj6m223?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfn3bpc23" data-bluesky-cid="bafyreih7dwnh4djhrbhpci7rlseybjok4qbi7u52gjtwv6pzcrr46bs5au"><p lang="en">Step 1: labels not legends. This makes the graph much easier to read.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfn3bpc23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfnklws23" data-bluesky-cid="bafyreicxbhyxwiaxxkkbbxypwqarqrwve7u4sdcsuryujmtx6y7dhli7qi"><p lang="en">Step 2: all text horizontal. Horizontal text is easier to read than vertical text.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfnklws23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfo2rjc23" data-bluesky-cid="bafyreic7ihkvkmveyjt6gfk7wabush7vl7k5dym3z6emltvowaepmgzfgi"><p lang="en">Step 3: eliminated unnecessary ink.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfo2rjc23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfoqqnc23" data-bluesky-cid="bafyreidkwcxj6x7gvzeeyrgtlnxmomeka25dtyqxb47xorzmqkuyhfvqbm"><p lang="en">That's Stata, here's a start in R.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfoqqnc23?ref_src=embed">March 11, 2025</a></blockquote>

  <script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>

  <p>The event study plot above follows these principles. Here's a summary of the key steps:</p>

  <ol>
    <li><strong>Remove clutter.</strong> No top/right borders (<code>graphregion(color(white))</code>), no unnecessary gridlines, no legend when you can label directly.</li>
    <li><strong>Use a single accent color</strong> for your main result. Use gray for reference lines and context. The code above uses <code>mcolor("44 95 138")</code> — a muted blue.</li>
    <li><strong>Shaded confidence bands are cleaner than error bars</strong> for continuous data. In Stata, use <code>rarea</code> for shaded bands instead of <code>rcap</code> for error bars. The event study code above demonstrates this — the <code>twoway rarea</code> command creates the shaded CI bands, while <code>connected</code> plots the point estimates.</li>
    <li><strong>Informative axis labels.</strong> "Years Relative to Policy Adoption" not "period". Use <code>xtitle()</code> and <code>ytitle()</code> with readable sizes.</li>
    <li><strong>Remove the plot title.</strong> Economics papers put the title in the figure caption, not on the plot itself.</li>
    <li><strong>Export at high resolution.</strong> Use <code>graph export "file.png", width(2400)</code> or export as PDF for vector graphics.</li>
  </ol>

  {% capture callout_content %}<ul>
      <li>Can you read all text when the figure is printed at its final size?</li>
      <li>Are axis labels descriptive? (not variable names)</li>
      <li>Is the aspect ratio wider than tall? (roughly 4:3 or 16:10)</li>
      <li>Is the resolution at least 300 DPI for print?</li>
      <li>Did you export as PDF (vector) or high-res PNG?</li>
      <li>Is the background white? (no gray default)</li>
      <li>Are confidence intervals shown as shaded bands (not error bars) where appropriate?</li>
    </ul>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "Figure Checklist", content: callout_content %}
</section>

<!-- ============================================================ -->
<!-- PART E: PROFESSIONAL DD TABLE WITH SUBGROUPS -->
<!-- ============================================================ -->
<section id="dd-table" class="tutorial-section">
  <h2>Part F: Professional DD Table with Subgroups</h2>

  <p>A strong empirical paper doesn't just report the main result. It shows robustness across specifications and explores heterogeneity. Here we create a single table with: (1) the main result, (2) results by region subgroup, and (3) a specification with additional controls.</p>

  {% capture sc %}use "$build/output/analysis_panel.dta", clear

* Label variables for table output
label variable treated "Treated"
label variable log_pop "Log Population"
label variable median_income "Median Income"

* Col 1: Main result
reghdfe fatal_crashes treated, absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m1

* Col 2: With controls
reghdfe fatal_crashes treated log_pop median_income, ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m2

* Col 3: South only
reghdfe fatal_crashes treated log_pop if region == "South", ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m3

* Col 4: Non-South
reghdfe fatal_crashes treated log_pop if region != "South", ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m4

* Col 5: Serious crashes as outcome
reghdfe serious_crashes treated log_pop, ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m5

* Export professional table
esttab m1 m2 m3 m4 m5 ///
    using "$analysis/output/tables/dd_results.tex", replace ///
    se(%9.3f) b(%9.3f) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    nomtitles nonumbers ///
    label fragment ///
    prehead("\begin{tabular}{l*{5}{c}}" ///
            "\midrule \midrule" ///
            "Dependent Variables: &\multicolumn{4}{c}{Fatal Crashes} &Serious\\" ///
            "\cmidrule(lr){2-5} \cmidrule(lr){6-6}" ///
            "Sample: & All & All & South & Non-South & All \\" ///
            "Model: & (1) & (2) & (3) & (4) & (5) \\" ///
            "\midrule" ///
            "\emph{Variables} \\") ///
    posthead("") ///
    prefoot("\midrule" ///
            "\emph{Fixed-effects} \\") ///
    stats(state_fe year_fe N r2, ///
        labels("State FE" "Year FE" ///
               "\midrule \emph{Fit statistics} \\ Observations" ///
               "R\$^2\$") ///
        fmt(%s %s %9.0fc %9.3f)) ///
    postfoot("\midrule \midrule" ///
             "\multicolumn{6}{l}{\emph{Clustered (state) standard-errors in parentheses}}\\" ///
             "\multicolumn{6}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\" ///
             "\end{tabular}"){% endcapture %}
  {% capture rc %}pacman::p_load(fixest, data.table)

panel <- fread("build/output/analysis_panel.csv")

# Col 1: Main result
m1 <- feols(fatal_crashes ~ treated | state_fips + year,
            data = panel, vcov = ~state_fips)

# Col 2: With controls
m2 <- feols(fatal_crashes ~ treated + log_pop + median_income
            | state_fips + year,
            data = panel, vcov = ~state_fips)

# Col 3: South only
m3 <- feols(fatal_crashes ~ treated + log_pop | state_fips + year,
            data = panel[region == "South"], vcov = ~state_fips)

# Col 4: Non-South
m4 <- feols(fatal_crashes ~ treated + log_pop | state_fips + year,
            data = panel[region != "South"], vcov = ~state_fips)

# Col 5: Serious crashes as outcome
m5 <- feols(serious_crashes ~ treated + log_pop | state_fips + year,
            data = panel, vcov = ~state_fips)

# Export professional table
etable(
  m1, m2, m3, m4, m5,
  tex = TRUE, file = "analysis/output/tables/dd_table.tex", replace = TRUE,
  headers = list(
    ":_sym:" = c("(1)", "(2)", "(3)", "(4)", "(5)")
  ),
  style.tex = style.tex(
    depvar.title = "Dep. Var.:",
    fixef.title = "\\midrule",
    yesNo = c("Yes", ""),
    stats.title = "\\midrule"
  ),
  fitstat = ~ n + r2 + wr2,
  dict = c(
    fatal_crashes = "Fatal Crashes",
    serious_crashes = "Serious Crashes",
    treated = "Policy Adopted",
    log_pop = "Log(Population)",
    median_income = "Median Income"
  ),
  se.below = TRUE, depvar = TRUE
){% endcapture %}
  {% capture pc %}import pyfixest as pf
import pandas as pd

panel = pd.read_csv("build/output/analysis_panel.csv")

# Col 1: Main result
m1 = pf.feols("fatal_crashes ~ treated | state_fips + year",
              vcov={"CRV1": "state_fips"}, data=panel)

# Col 2: With controls
m2 = pf.feols("fatal_crashes ~ treated + log_pop + median_income"
              " | state_fips + year",
              vcov={"CRV1": "state_fips"}, data=panel)

# Col 3: South only
m3 = pf.feols("fatal_crashes ~ treated + log_pop | state_fips + year",
              vcov={"CRV1": "state_fips"},
              data=panel[panel['region'] == 'South'])

# Col 4: Non-South
m4 = pf.feols("fatal_crashes ~ treated + log_pop | state_fips + year",
              vcov={"CRV1": "state_fips"},
              data=panel[panel['region'] != 'South'])

# Col 5: Serious crashes as outcome
m5 = pf.feols("serious_crashes ~ treated + log_pop | state_fips + year",
              vcov={"CRV1": "state_fips"}, data=panel)

# Export table
pf.etable(
    [m1, m2, m3, m4, m5],
    type="tex",
    file="analysis/output/tables/dd_table.tex",
    labels={
        "fatal_crashes": "Fatal Crashes",
        "serious_crashes": "Serious Crashes",
        "treated": "Policy Adopted",
        "log_pop": "Log(Population)",
        "median_income": "Median Income",
    },
){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <p>Here's the DD regression table produced by <code>esttab</code>:</p>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/dd_results.png" alt="DD regression table">
  </figure>
</section>

<!-- ============================================================ -->
<!-- PART F: INSTRUMENTAL VARIABLES -->
<!-- ============================================================ -->
<section id="iv-analysis" class="tutorial-section">
  <h2>Part G: Instrumental Variables</h2>

  <p>This section uses a separate, pre-built dataset (<code>iv_data.csv</code>) inspired by Angrist & Lavy (1999). We estimate the effect of class size on test scores, using a Maimonides-rule instrument (enrollment determines class size via a maximum-of-40 rule).</p>

  <h3>The Setup</h3>
  <ul>
    <li><strong>Endogenous variable:</strong> class_size (correlated with school quality)</li>
    <li><strong>Instrument:</strong> enrollment (determines class size via the Maimonides rule, but plausibly doesn't directly affect test scores)</li>
    <li><strong>Outcome:</strong> test_score</li>
  </ul>

  <h3>First Stage</h3>
  <p>Does the instrument predict the endogenous variable? Check the <strong>F-statistic</strong> — it should be well above 10.</p>

  {% capture sc %}import delimited "$analysis/code/iv_data.csv", clear

* Label variables
label variable class_size "Class Size"
label variable test_score "Test Score"
label variable enrollment "Enrollment"
label variable pct_disadvantaged "Pct. Disadvantaged"

* First stage: Does enrollment predict class size?
regress class_size enrollment pct_disadvantaged, vce(robust)
estimates store first_stage

* Check the F-stat on enrollment — should be >> 10
test enrollment{% endcapture %}
  {% capture rc %}pacman::p_load(fixest, data.table)

iv_data <- fread("analysis/code/iv_data.csv")

# First stage: Does enrollment predict class size?
first_stage <- feols(class_size ~ enrollment + pct_disadvantaged,
                     data = iv_data)
summary(first_stage)

# F-statistic on excluded instrument
cat("First-stage F:", fitstat(first_stage, "f")$f$stat, "\n"){% endcapture %}
  {% capture pc %}import pandas as pd
import statsmodels.formula.api as smf

iv_data = pd.read_csv("analysis/code/iv_data.csv")

# First stage: Does enrollment predict class size?
first_stage = smf.ols(
    'class_size ~ enrollment + pct_disadvantaged', data=iv_data
).fit(cov_type='HC1')
print(first_stage.summary())

# F-statistic on excluded instrument
f_test = first_stage.f_test('enrollment = 0')
print(f"\nFirst-stage F-statistic: {f_test.fvalue[0][0]:.1f}"){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <h3>Reduced Form</h3>
  <p>The reduced form regresses the outcome directly on the instrument. If the instrument affects the outcome, it's working through the endogenous variable.</p>

  {% capture sc %}* Reduced form: Does enrollment predict test scores?
regress test_score enrollment pct_disadvantaged, vce(robust)
estimates store reduced_form{% endcapture %}
  {% capture rc %}# Reduced form: Does enrollment predict test scores?
reduced_form <- feols(test_score ~ enrollment + pct_disadvantaged,
                      data = iv_data)
summary(reduced_form){% endcapture %}
  {% capture pc %}# Reduced form: Does enrollment predict test scores?
reduced_form = smf.ols(
    'test_score ~ enrollment + pct_disadvantaged', data=iv_data
).fit(cov_type='HC1')
print(reduced_form.summary()){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <h3>2SLS Estimation</h3>

  {% capture sc %}* OLS (biased estimate)
regress test_score class_size pct_disadvantaged, vce(robust)
estimates store ols

* 2SLS: Instrument class_size with enrollment
ivregress 2sls test_score pct_disadvantaged (class_size = enrollment), ///
    vce(robust) first
estimates store iv_2sls

* Export all four specifications
esttab first_stage reduced_form iv_2sls ols ///
    using "$analysis/output/tables/iv_results.tex", replace ///
    label fragment ///
    order(class_size enrollment pct_disadvantaged) ///
    b(%9.3f) se(%9.3f) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    nomtitles nonumbers ///
    prehead("\begin{tabular}{l*{4}{c}}" ///
            "\midrule \midrule" ///
            "Dependent Variable: & Class Size & \multicolumn{3}{c}{Test Score}\\" ///
            "\cmidrule(lr){2-2} \cmidrule(lr){3-5}" ///
            "Model: & First Stage & Reduced Form & 2SLS & OLS\\" ///
            "& (1) & (2) & (3) & (4)\\" ///
            "\midrule" ///
            "\emph{Variables} \\") ///
    posthead("") ///
    prefoot("\midrule" ///
            "\emph{Fit statistics} \\") ///
    stats(N r2, fmt(%9.0fc %9.3f) ///
          labels("Observations" "R\$^2\$")) ///
    postfoot("\midrule \midrule" ///
             "\multicolumn{5}{l}{\emph{Heteroskedasticity-robust standard-errors in parentheses}}\\" ///
             "\multicolumn{5}{l}{\emph{Instrument: School enrollment}}\\" ///
             "\multicolumn{5}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\" ///
             "\end{tabular}"){% endcapture %}
  {% capture rc %}# 2SLS: Instrument class_size with enrollment
# fixest syntax: outcome ~ controls | FE | endogenous ~ instrument
iv_model <- feols(
  test_score ~ pct_disadvantaged | 0 | class_size ~ enrollment,
  data = iv_data
)
summary(iv_model)

# First-stage F
cat("First-stage F:", fitstat(iv_model, "ivf")$ivf$stat, "\n")

# Compare OLS vs IV
ols_model <- feols(test_score ~ class_size + pct_disadvantaged,
                   data = iv_data)
etable(ols_model, iv_model,
       dict = c(class_size = "Class Size",
                fit_class_size = "Class Size (IV)")){% endcapture %}
  {% capture pc %}from linearmodels.iv import IV2SLS

# 2SLS: Instrument class_size with enrollment
iv_model = IV2SLS.from_formula(
    'test_score ~ 1 + pct_disadvantaged + [class_size ~ enrollment]',
    data=iv_data
).fit(cov_type='robust')
print(iv_model.summary)

# First-stage F
print(f"\nFirst-stage F: "
      f"{iv_model.first_stage.diagnostics['f.stat'].stat:.1f}")

# Compare: OLS
ols = smf.ols(
    'test_score ~ class_size + pct_disadvantaged', data=iv_data
).fit(cov_type='HC1')
print("\n--- OLS ---")
print(f"Class size coef: {ols.params['class_size']:.3f}")
print("\n--- IV ---")
print(f"Class size coef: {iv_model.params['class_size']:.3f}"){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <p>Here's the IV results table comparing OLS, first stage, reduced form, and 2SLS:</p>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/iv_results.png" alt="IV regression table">
  </figure>

  {% capture callout_content %}<p>OLS is biased because class size is correlated with school quality (omitted variable bias). The IV estimate isolates the causal effect by using only variation in class size driven by enrollment thresholds. The IV estimate should typically be larger in magnitude if OLS is biased toward zero (attenuation bias from measurement error) or could go either way depending on the source of endogeneity.</p>{% endcapture %}
  {% include "callout.liquid", type: "warning", heading: "OLS vs. IV: What to Expect", content: callout_content %}
</section>

<!-- ============================================================ -->
<!-- PART G: REGRESSION DISCONTINUITY -->
<!-- ============================================================ -->
<section id="rd-analysis" class="tutorial-section">
  <h2>Part H: Regression Discontinuity</h2>

  <p>This section uses a pre-built dataset (<code>rd_data.csv</code>) inspired by Carpenter & Dobkin (2009). We estimate the effect of legal drinking access (turning 21) on mortality using a sharp RD design.</p>

  <h3>The Setup</h3>
  <ul>
    <li><strong>Running variable:</strong> days_from_21 (days relative to 21st birthday)</li>
    <li><strong>Cutoff:</strong> 0 (turning 21)</li>
    <li><strong>Treatment:</strong> over_21 (legal access to alcohol)</li>
    <li><strong>Outcome:</strong> mortality_rate</li>
  </ul>

  <h3>Linear Specification</h3>
  <p>The simplest RD: a linear function of the running variable on each side of the cutoff, within a bandwidth.</p>

  {% capture sc %}import delimited "$analysis/code/rd_data.csv", clear

* Label variables
label variable mortality_rate "Mortality Rate"
label variable over_21 "Over 21"
label variable days_from_21 "Days from 21st Birthday"

* Create interaction term
gen days_x_over21 = days_from_21 * over_21

* Linear RD within bandwidth of 365 days
regress mortality_rate over_21 days_from_21 days_x_over21 ///
    if abs(days_from_21) <= 365, vce(robust)
estimates store rd_linear

* The coefficient on over_21 is the RD estimate (jump at cutoff){% endcapture %}
  {% capture rc %}pacman::p_load(fixest, data.table)

rd_data <- fread("analysis/code/rd_data.csv")

# Linear RD within bandwidth of 365 days
rd_linear <- feols(
  mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21,
  data = rd_data[abs(days_from_21) <= 365]
)
summary(rd_linear)

# Narrower bandwidth (180 days)
rd_narrow <- feols(
  mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21,
  data = rd_data[abs(days_from_21) <= 180]
)
summary(rd_narrow){% endcapture %}
  {% capture pc %}import pandas as pd
import statsmodels.formula.api as smf

rd_data = pd.read_csv("analysis/code/rd_data.csv")

# Linear RD within bandwidth of 365 days
subset = rd_data[rd_data['days_from_21'].abs() <= 365]
rd_linear = smf.ols(
    'mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21',
    data=subset
).fit(cov_type='HC1')
print("=== Linear RD (bw=365) ===")
print(rd_linear.summary())

# Narrower bandwidth (180 days)
subset_narrow = rd_data[rd_data['days_from_21'].abs() <= 180]
rd_narrow = smf.ols(
    'mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21',
    data=subset_narrow
).fit(cov_type='HC1')
print("\n=== Linear RD (bw=180) ===")
print(f"RD estimate: {rd_narrow.params['over_21']:.3f} "
      f"(SE: {rd_narrow.bse['over_21']:.3f})"){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <h3>Polynomial Specification</h3>
  <p>A polynomial specification allows for curvature in the relationship between the running variable and the outcome. This can be more flexible but risks overfitting.</p>

  {% capture sc %}* Create polynomial terms
gen days_sq = days_from_21^2
gen days_cu = days_from_21^3
gen days_sq_x_over21 = days_sq * over_21
gen days_cu_x_over21 = days_cu * over_21

* Quadratic RD
regress mortality_rate over_21 days_from_21 days_x_over21 ///
    days_sq days_sq_x_over21 ///
    if abs(days_from_21) <= 365, vce(robust)
estimates store rd_quadratic

* Cubic RD
regress mortality_rate over_21 days_from_21 days_x_over21 ///
    days_sq days_sq_x_over21 days_cu days_cu_x_over21 ///
    if abs(days_from_21) <= 365, vce(robust)
estimates store rd_cubic

* Export results table
esttab rd_linear rd_quadratic rd_cubic ///
    using "$analysis/output/tables/rd_results.tex", replace ///
    keep(over_21) label fragment ///
    b(%9.4f) se(%9.4f) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    nomtitles nonumbers ///
    prehead("\begin{tabular}{l*{3}{c}}" ///
            "\midrule \midrule" ///
            "Dependent Variable: &\multicolumn{3}{c}{Mortality Rate}\\" ///
            "\cmidrule(lr){2-4}" ///
            "Model:              & Linear & Quadratic & Cubic\\" ///
            "                    & (1) & (2) & (3)\\" ///
            "\midrule" ///
            "\emph{Variables} \\") ///
    posthead("") ///
    prefoot("\midrule" ///
            "\emph{Fit statistics} \\") ///
    stats(N r2, fmt(%9.0fc %9.4f) ///
          labels("Observations" "R\$^2\$")) ///
    postfoot("\midrule \midrule" ///
             "\multicolumn{4}{l}{\emph{Robust standard-errors in parentheses}}\\" ///
             "\multicolumn{4}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\" ///
             "\end{tabular}"){% endcapture %}
  {% capture rc %}pacman::p_load(fixest, rdrobust, data.table)

rd_data <- fread("analysis/code/rd_data.csv")

# Quadratic RD
rd_data[, days2 := days_from_21^2]
rd_quad <- feols(
  mortality_rate ~ over_21 + days_from_21 + days2 +
    over_21:days_from_21 + over_21:days2,
  data = rd_data[abs(days_from_21) <= 365]
)
summary(rd_quad)

# Using rdrobust for optimal bandwidth selection
rd_robust <- rdrobust(
  y = rd_data$mortality_rate,
  x = rd_data$days_from_21,
  c = 0
)
summary(rd_robust)

# RD plot
rdplot(
  y = rd_data$mortality_rate,
  x = rd_data$days_from_21,
  c = 0,
  title = "RD Plot: Mortality at Age 21",
  x.label = "Days from 21st Birthday",
  y.label = "Mortality Rate"
){% endcapture %}
  {% capture pc %}import numpy as np
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf

rd_data = pd.read_csv("analysis/code/rd_data.csv")

# Quadratic RD
rd_data['days2'] = rd_data['days_from_21'] ** 2
subset = rd_data[rd_data['days_from_21'].abs() <= 365]
rd_quad = smf.ols(
    'mortality_rate ~ over_21 + days_from_21 + days2'
    ' + over_21:days_from_21 + over_21:days2',
    data=subset
).fit(cov_type='HC1')
print("=== Quadratic RD ===")
print(f"RD estimate: {rd_quad.params['over_21']:.3f} "
      f"(SE: {rd_quad.bse['over_21']:.3f})")

# RD Plot
fig, ax = plt.subplots(figsize=(8, 5))
# Bin the data
bins = np.arange(-365, 366, 30)
rd_data['bin'] = pd.cut(rd_data['days_from_21'], bins=bins)
binned = rd_data.groupby('bin')['mortality_rate'].mean().reset_index()
binned['mid'] = binned['bin'].apply(lambda x: x.mid if pd.notna(x) else np.nan)
binned = binned.dropna()

ax.scatter(binned['mid'], binned['mortality_rate'], s=20, alpha=0.7)
ax.axvline(0, color='red', linestyle='--', linewidth=0.8)
ax.set_xlabel('Days from 21st Birthday')
ax.set_ylabel('Mortality Rate')
ax.set_title('RD Plot: Mortality at Age 21')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
fig.tight_layout()
fig.savefig('analysis/output/figures/rd_plot.png', dpi=300){% endcapture %}
  {% include "code-block.liquid", stata: sc, r: rc, python: pc %}

  <p>Here's the RD plot and regression table:</p>
  <figure class="output-figure narrow">
    <img src="data/project-walkthrough/figures/rd_plot.png" alt="RD plot with linear fit">
    <figcaption>RD plot with linear fit showing a clear discontinuity in mortality at age 21.</figcaption>
  </figure>
  <figure class="output-figure narrow">
    <img src="data/project-walkthrough/figures/rd_plot_poly.png" alt="RD plot with polynomial fit">
    <figcaption>RD plot with polynomial fit. The discontinuity is robust to allowing curvature in the running variable.</figcaption>
  </figure>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/rd_results.png" alt="RD regression table">
  </figure>

  {% capture callout_content %}<p>Start with <strong>linear</strong> — it's simpler and less prone to overfitting. Use polynomial specifications as robustness checks. If the linear and polynomial estimates are very different, it suggests the result may be sensitive to functional form assumptions. The <code>rdrobust</code> package provides data-driven bandwidth selection and bias-corrected inference, and is the current best practice.</p>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "Linear vs. Polynomial: Which to Use?", content: callout_content %}
</section>

<!-- ============================================================ -->
<!-- EXERCISES -->
<!-- ============================================================ -->
<section id="exercises" class="tutorial-section">
  <h2>Exercises</h2>

  <p>Practice the techniques from this walkthrough. Each ZIP contains the exercise PDF, a codebook, and all necessary datasets.</p>

  {% capture ex1_content %}<div style="text-align: center;">
    <p><strong>Exercise 1</strong> — Data cleaning, merging, descriptive statistics, and difference-in-differences.</p>
    <div style="display: flex; gap: 12px; justify-content: center; flex-wrap: wrap;">
      <a href="data/exercises/exercise1.zip" download class="download-btn">Exercise 1</a>
      <a href="data/exercises/exercise1-solutions.zip" download class="download-btn">Solutions</a>
    </div>
  </div>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "Exercise 1", content: ex1_content %}

  {% capture ex2_content %}<div style="text-align: center;">
    <p><strong>Exercise 2</strong> — Merging, two-way fixed effects, event studies, and instrumental variables.</p>
    <div style="display: flex; gap: 12px; justify-content: center; flex-wrap: wrap;">
      <a href="data/exercises/exercise2.zip" download class="download-btn">Exercise 2</a>
      <a href="data/exercises/exercise2-solutions.zip" download class="download-btn">Solutions</a>
    </div>
  </div>{% endcapture %}
  {% include "callout.liquid", type: "info", heading: "Exercise 2", content: ex2_content %}
</section>

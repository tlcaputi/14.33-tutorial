---
layout: layouts/tutorial.html
title: Complete Project Walkthrough | 14.33
current_page: project-walkthrough
---
<h1>Complete Project Walkthrough</h1>
<p class="subtitle">From raw data to publication-ready output — start to finish</p>

<p>This walkthrough takes you through an entire empirical project: loading data, cleaning, merging multiple datasets, running a difference-in-differences analysis with event study, and producing publication-quality tables and figures. We also cover instrumental variables and regression discontinuity as standalone analyses.</p>

<!-- ============================================================ -->
<!-- PROJECT DIRECTORY SETUP -->
<!-- ============================================================ -->
<section id="project-setup" class="tutorial-section">
  <h2>Project Setup</h2>

  <p>An empirical project separates <strong>raw data</strong> from <strong>processed data</strong>, and <strong>build code</strong> (data processing) from <strong>analysis code</strong> (regressions, tables, figures). The download below contains the full project with this structure already in place.</p>

  <div class="callout-info" style="text-align: center;">
    <h4>Download the Project</h4>
    <p><strong>Starter</strong> packages contain data and empty code files for you to fill in as you follow along. <strong>Complete</strong> packages include finished code that runs end-to-end.</p>
    <table style="margin: 16px auto; border-collapse: collapse; text-align: center;">
      <thead>
        <tr>
          <th style="padding: 6px 16px;"></th>
          <th style="padding: 6px 16px;">Stata</th>
          <th style="padding: 6px 16px;">R</th>
          <th style="padding: 6px 16px;">Python</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="padding: 6px 16px; font-weight: 600;">Starter</td>
          <td style="padding: 6px 16px;"><a href="data/pkg-stata.zip" download class="download-btn">Download</a></td>
          <td style="padding: 6px 16px;"><a href="data/pkg-r.zip" download class="download-btn">Download</a></td>
          <td style="padding: 6px 16px;"><a href="data/pkg-python.zip" download class="download-btn">Download</a></td>
        </tr>
        <tr>
          <td style="padding: 6px 16px; font-weight: 600;">Complete</td>
          <td style="padding: 6px 16px;"><a href="data/pkg-stata-complete.zip" download class="download-btn">Download</a></td>
          <td style="padding: 6px 16px;"><a href="data/pkg-r-complete.zip" download class="download-btn">Download</a></td>
          <td style="padding: 6px 16px;"><a href="data/pkg-python-complete.zip" download class="download-btn">Download</a></td>
        </tr>
      </tbody>
    </table>
    <p style="margin-top: 12px;"><a href="data/project-walkthrough/project_walkthrough_slides.pdf" download class="download-btn">Tutorial Slides (PDF)</a></p>
  </div>

  <h3>Running Your Scripts</h3>

  <p>Once you've downloaded and unzipped the project, open a terminal in the project folder and run the master file. You can also run individual scripts for development.</p>

  <p><strong>Stata (command line):</strong></p>
  <ul>
    <li>Mac: <code>/Applications/Stata/StataMP.app/Contents/MacOS/stata-mp -b do master.do</code></li>
    <li>Windows: <code>"C:\Program Files\Stata18\StataMP-64.exe" /e do master.do</code></li>
    <li>Linux: <code>stata-mp -b do master.do</code></li>
  </ul>
  <p>The <code>-b</code> (Mac/Linux) or <code>/e</code> (Windows) flag runs Stata in batch mode — output goes to a <code>.log</code> file in the working directory instead of the GUI.</p>

  <p><strong>Stata (VS Code):</strong> Install the <a href="https://marketplace.visualstudio.com/items?itemName=kylebarron.stata-enhanced">Stata Enhanced</a> extension for syntax highlighting and <a href="https://marketplace.visualstudio.com/items?itemName=Yeaoh.stataRun">stataRun</a> to run code directly. After installing stataRun, open VS Code settings and set the path to your Stata executable (e.g. <code>/Applications/Stata/StataMP.app/Contents/MacOS/stata-mp</code>). Then open a <code>.do</code> file and press <kbd>Ctrl+Shift+A</kbd> (<kbd>Cmd+Shift+A</kbd> on Mac) to run the entire file.</p>

  <p><strong>R:</strong> Run <code>Rscript master.R</code> from the terminal, or open the project in RStudio and source the master file.</p>

  <p><strong>Python:</strong> Run <code>python master.py</code> from the terminal, or use VS Code with the Python extension to run scripts interactively.</p>

  <h3>What's Inside</h3>

  <p>Unzipping gives you a single project folder (e.g. <code>pkg-stata/</code>) with this structure:</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Directory Structure</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code>pkg-stata/
├── master.do                  ← Runs everything in order
├── build/
│   ├── code/
│   │   ├── 01_filter_crashes.do
│   │   ├── 02_collapse_crashes.do
│   │   ├── 03_reshape_crashes.do
│   │   ├── 04_append_demographics.do
│   │   ├── 05_collapse_demographics.do
│   │   └── 06_merge_datasets.do
│   ├── input/                 ← Raw data (NEVER modified by code)
│   │   ├── crash_data.csv
│   │   ├── demographic_survey/  ← 21 annual CSVs (1995-2015)
│   │   ├── policy_adoptions.csv
│   │   └── state_names.csv
│   └── output/                ← Processed data (created by build scripts)
└── analysis/
    ├── code/
    │   ├── 01_descriptive_table.do
    │   ├── 02_dd_regression.do
    │   ├── 03_event_study.do
    │   ├── 04_dd_table.do
    │   ├── 05_iv.do
    │   └── 06_rd.do
    └── output/
        ├── tables/
        └── figures/</code></pre>
  </div>

  <h3>Key Principles</h3>
  <ul>
    <li><strong>Raw data is read-only.</strong> Files in <code>build/input/</code> are never modified by your code. If you need to clean or transform data, save the result to <code>build/output/</code>.</li>
    <li><strong>Build vs. analysis.</strong> Build scripts (<code>build/code/</code>) turn raw data into analysis-ready datasets. Analysis scripts (<code>analysis/code/</code>) take those datasets and produce results. This separation means you can change a regression without re-processing all your data.</li>
    <li><strong>Numbered scripts.</strong> Prefix scripts with <code>01_</code>, <code>02_</code>, etc. so the execution order is obvious. A <code>master.do</code> file at the root runs them all sequentially.</li>
    <li><strong>Outputs are reproducible.</strong> Everything in <code>build/output/</code> and <code>analysis/output/</code> can be regenerated by running <code>master.do</code>. Never hand-edit output files.</li>
  </ul>

  <h3>The Master File</h3>
  <p>The <code>master.do</code> sets global paths and runs every script in order. Anyone should be able to replicate your results by changing one path and running this file.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* MASTER DO FILE
clear all
set more off

* >>> CHANGE THIS to your project path <<<
global root "/path/to/pkg-stata"
global build "$root/build"
global analysis "$root/analysis"

cd "$root"

* Build stage
do "$build/code/01_filter_crashes.do"
do "$build/code/02_collapse_crashes.do"
do "$build/code/03_reshape_crashes.do"
do "$build/code/04_append_demographics.do"
do "$build/code/05_collapse_demographics.do"
do "$build/code/06_merge_datasets.do"

* Analysis stage
do "$analysis/code/01_descriptive_table.do"
do "$analysis/code/02_dd_regression.do"
do "$analysis/code/03_event_study.do"
do "$analysis/code/04_dd_table.do"
do "$analysis/code/05_iv.do"
do "$analysis/code/06_rd.do"</code></pre>
    <pre class="code-content r"><code class="language-r"># master.R
rm(list = ls())

# Set root directory (CHANGE THIS to your project path)
root <- "."
build <- file.path(root, "build")
analysis <- file.path(root, "analysis")

# Build stage
source(file.path(build, "code", "01_filter_crashes.R"))
source(file.path(build, "code", "02_collapse_crashes.R"))
source(file.path(build, "code", "03_reshape_crashes.R"))
source(file.path(build, "code", "04_append_demographics.R"))
source(file.path(build, "code", "05_collapse_demographics.R"))
source(file.path(build, "code", "06_merge_datasets.R"))

# Analysis stage
source(file.path(analysis, "code", "01_descriptive_table.R"))
source(file.path(analysis, "code", "02_dd_regression.R"))
source(file.path(analysis, "code", "03_event_study.R"))
source(file.path(analysis, "code", "04_dd_table.R"))
source(file.path(analysis, "code", "05_iv.R"))
source(file.path(analysis, "code", "06_rd.R"))</code></pre>
    <pre class="code-content python"><code class="language-python"># master.py
import subprocess, sys, os

root = "."
build = os.path.join(root, "build")
analysis = os.path.join(root, "analysis")

scripts = [
    f"{build}/code/01_filter_crashes.py",
    f"{build}/code/02_collapse_crashes.py",
    f"{build}/code/03_reshape_crashes.py",
    f"{build}/code/04_append_demographics.py",
    f"{build}/code/05_collapse_demographics.py",
    f"{build}/code/06_merge_datasets.py",
    f"{analysis}/code/01_descriptive_table.py",
    f"{analysis}/code/02_dd_regression.py",
    f"{analysis}/code/03_event_study.py",
    f"{analysis}/code/04_dd_table.py",
    f"{analysis}/code/05_iv.py",
    f"{analysis}/code/06_rd.py",
]

for script in scripts:
    print(f"Running {script}...")
    subprocess.run([sys.executable, script], check=True)</code></pre>
  </div>
</section>

<!-- ============================================================ -->
<!-- DATA CLEANING INTRO -->
<!-- ============================================================ -->
<section class="tutorial-section">
  <p>Every raw dataset is a little different, and you'll need to figure out what steps are required to get each one into the format your analysis needs. Parts A and B walk through two examples — cleaning crash data and cleaning demographic survey data — that illustrate common operations (filtering, collapsing, reshaping, looping, appending, destringing). The specific steps will depend on the raw data you're actually working with.</p>
</section>

<!-- ============================================================ -->
<!-- PART A: CLEANING THE CRASH DATA -->
<!-- ============================================================ -->
<section id="crash-data" class="tutorial-section">
  <h2>Part A: Cleaning the Crash Data</h2>

  <p>The crash data has one row per crash, but our analysis is at the <strong>state-year</strong> level. Scripts 01–03 read the raw data, filter it, collapse it to counts, and reshape it so each severity type gets its own column.</p>

  <h3>Step 1: Read and Filter Crashes</h3>
  <p>Start by reading the crash-level CSV. This is granular data we'll need to aggregate.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* Read crash-level data (CSV)
import delimited "$build/input/crash_data.csv", clear
describe
summarize
tab severity</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(data.table)

# Read crash-level data (CSV)
crashes <- fread("build/input/crash_data.csv")
str(crashes)
summary(crashes)
table(crashes$severity)</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd

# Read crash-level data (CSV)
crashes = pd.read_csv("build/input/crash_data.csv")
print(crashes.info())
print(crashes.describe())
print(crashes['severity'].value_counts())</code></pre>
  </div>

  <p>Script <code>01_filter_crashes</code> also drops observations we don't need before any aggregation.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* Load crash data
import delimited "$build/input/crash_data.csv", clear

* --- FILTER: Keep only fatal and serious crashes ---
drop if severity == "minor"

save "$build/output/crashes_filtered.dta", replace</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(data.table)

# Load crash data
crashes <- fread("build/input/crash_data.csv")

# Filter: keep only fatal and serious crashes
crashes <- crashes[severity != "minor"]

fwrite(crashes, "build/output/crashes_filtered.csv")</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd

# Load crash data
crashes = pd.read_csv("build/input/crash_data.csv")

# Filter: keep only fatal and serious crashes
crashes = crashes[crashes['severity'] != 'minor']

crashes.to_csv("build/output/crashes_filtered.csv", index=False)</code></pre>
  </div>

  <h3>Step 2: Collapse to State-Year-Severity</h3>
  <p>Script <code>02_collapse_crashes</code> counts crashes by state, year, and severity type.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/crashes_filtered.dta", clear

* --- COLLAPSE: Count crashes by state-year-severity (long format) ---
gen one = 1
collapse (sum) n_crashes = one, by(state_fips year severity)

save "$build/output/crashes_long.dta", replace</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(data.table)

crashes <- fread("build/output/crashes_filtered.csv")

# Collapse to state-year-severity level (long format)
crashes_long <- crashes[, .(n_crashes = .N), by = .(state_fips, year, severity)]

fwrite(crashes_long, "build/output/crashes_long.csv")</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd

crashes = pd.read_csv("build/output/crashes_filtered.csv")

# Collapse to state-year-severity level (long format)
crashes_long = (crashes.groupby(['state_fips', 'year', 'severity'])
                .size().reset_index(name='n_crashes'))

crashes_long.to_csv("build/output/crashes_long.csv", index=False)</code></pre>
  </div>

  <h3>Step 3: Reshape and Save</h3>
  <p>Script <code>03_reshape_crashes</code> pivots the long data wide, renames columns, computes derived variables, and saves the final crash dataset.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/crashes_long.dta", clear

* --- RESHAPE: Wide so each severity type becomes its own column ---
reshape wide n_crashes, i(state_fips year) j(severity) string

* Rename for clarity
rename n_crashesfatal fatal_crashes
rename n_crashesserious serious_crashes

* Compute total and fatal share
gen total_crashes = fatal_crashes + serious_crashes
gen fatal_share = fatal_crashes / total_crashes

* Save intermediate file
save "$build/output/crashes_state_year.dta", replace
describe</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(data.table)

crashes_long <- fread("build/output/crashes_long.csv")

# Reshape from long to wide: one column per severity type
crashes_sy <- dcast(crashes_long, state_fips + year ~ severity,
                    value.var = "n_crashes", fill = 0L)

# Rename and compute totals
setnames(crashes_sy, c("fatal", "serious"), c("fatal_crashes", "serious_crashes"))
crashes_sy[, total_crashes := fatal_crashes + serious_crashes]
crashes_sy[, fatal_share := fatal_crashes / total_crashes]

# Save intermediate file
fwrite(crashes_sy, "build/output/crashes_state_year.csv")
str(crashes_sy)</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd

crashes_long = pd.read_csv("build/output/crashes_long.csv")

# Reshape (pivot) from long to wide: one column per severity type
crashes_sy = crashes_long.pivot_table(
    index=['state_fips', 'year'], columns='severity',
    values='n_crashes', fill_value=0
).reset_index()
crashes_sy.columns.name = None
crashes_sy = crashes_sy.rename(columns={'fatal': 'fatal_crashes',
                                         'serious': 'serious_crashes'})

# Compute total and fatal share
crashes_sy['total_crashes'] = crashes_sy['fatal_crashes'] + crashes_sy['serious_crashes']
crashes_sy['fatal_share'] = crashes_sy['fatal_crashes'] / crashes_sy['total_crashes']

# Save intermediate file
crashes_sy.to_csv("build/output/crashes_state_year.csv", index=False)
print(crashes_sy.info())</code></pre>
  </div>

  <div class="callout-success">
    <h4>Why Save Intermediate Files?</h4>
    <p>Separating your build code (data processing) from analysis code is a key principle of project organization. Save collapsed data so you can modify your analysis without re-running expensive data processing steps every time.</p>
  </div>
</section>

<!-- ============================================================ -->
<!-- PART B: CLEANING THE DEMOGRAPHICS DATA -->
<!-- ============================================================ -->
<section id="demographics" class="tutorial-section">
  <h2>Part B: Cleaning the Demographics Data</h2>

  <p>The demographic survey data arrives as 21 separate annual CSV files (one per year, 1995–2015). Scripts 04 and 05 stack them into a single dataset, clean the data, and collapse to the state-year level.</p>

  <div class="callout-info">
    <h4>Why Loop and Append?</h4>
    <p>A lot of public-use data is released as separate annual files. You'll need to loop over the files and append them together before you can do anything else. Writing a loop is far better than manually importing 21 files — it's shorter, less error-prone, and generalizes if files are added later.</p>
  </div>

  <h3>Step 4: Loop and Append Demographics</h3>
  <p>Script <code>04_append_demographics</code> reads each annual CSV, tags it with the year, and appends all 21 files into one dataset.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* Loop through yearly survey CSV files and append
clear
forvalues y = 1995/2015 {
    preserve
    import delimited "$build/input/demographic_survey/demographic_survey_`y'.csv", clear
    gen year = `y'
    tempfile temp`y'
    save `temp`y''
    restore
    append using `temp`y''
}
save "$build/output/demographics_combined.dta", replace</code></pre>
    <pre class="code-content r"><code class="language-r"># Loop through yearly survey CSVs and stack them
files <- list.files("build/input/demographic_survey",
                    pattern = "^demographic_survey_\\d{4}\\.csv$",
                    full.names = TRUE)
demo_list <- lapply(files, function(f) {
  dt <- fread(f)
  dt[, year := as.integer(gsub(".*_(\\d{4})\\.csv$", "\\1", f))]
  dt
})
demo_all <- rbindlist(demo_list)
fwrite(demo_all, "build/output/demographics_combined.csv")</code></pre>
    <pre class="code-content python"><code class="language-python">import glob
import pandas as pd

# Read and stack all yearly survey CSVs
files = sorted(glob.glob("build/input/demographic_survey/demographic_survey_*.csv"))
frames = []
for f in files:
    df = pd.read_csv(f)
    year = int(f.split("_")[-1].replace(".csv", ""))
    df["year"] = year
    frames.append(df)
demo_all = pd.concat(frames, ignore_index=True)
demo_all.to_csv("build/output/demographics_combined.csv", index=False)</code></pre>
  </div>

  <h3>Step 5: Clean and Collapse Demographics</h3>
  <p>Script <code>05_collapse_demographics</code> drops DC and pre-2000 years, cleans the income variable (stored as a string with dollar signs and commas), and collapses to the state-year level using survey weights.</p>

  <div class="callout-info">
    <h4>Why Destring Income?</h4>
    <p>Survey data frequently stores numeric values as strings — "$45,000" instead of 45000. You must strip the formatting characters and convert to numeric before doing any math. This is one of the most common data-cleaning tasks in applied work.</p>
  </div>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/demographics_combined.dta", clear

* Drop DC and pre-2000 years
drop if state_fips == 51
drop if year < 2000

* Clean income: remove $ and commas, convert to numeric
destring income, replace ignore("$,")

* Collapse to state-year level using survey weights
collapse (rawsum) population = weight ///
    (mean) median_income = income pct_urban = urban ///
    [aweight = weight], by(state_fips year)

save "$build/output/demographics_state_year.dta", replace</code></pre>
    <pre class="code-content r"><code class="language-r">demo_all <- fread("build/output/demographics_combined.csv")

# Drop DC and pre-2000
demo_all <- demo_all[state_fips != 51 & year >= 2000]

# Clean income: remove $ and commas
demo_all[, income_num := as.numeric(gsub("[\\$,]", "", income))]

# Weighted collapse to state-year level
demographics <- demo_all[, .(
  population    = sum(weight),
  median_income = weighted.mean(income_num, weight),
  pct_urban     = weighted.mean(urban, weight)
), by = .(state_fips, year)]

fwrite(demographics, "build/output/demographics_state_year.csv")</code></pre>
    <pre class="code-content python"><code class="language-python">demo_all = pd.read_csv("build/output/demographics_combined.csv")

# Drop DC and pre-2000
demo_all = demo_all[(demo_all["state_fips"] != 51) & (demo_all["year"] >= 2000)]

# Clean income: remove $ and commas
demo_all["income_num"] = (demo_all["income"]
    .str.replace("$", "", regex=False)
    .str.replace(",", "")
    .astype(float))

# Weighted collapse
def weighted_mean(group, col, wt):
    return (group[col] * group[wt]).sum() / group[wt].sum()

demographics = (demo_all.groupby(["state_fips", "year"])
    .apply(lambda g: pd.Series({
        "population": g["weight"].sum(),
        "median_income": weighted_mean(g, "income_num", "weight"),
        "pct_urban": weighted_mean(g, "urban", "weight"),
    }))
    .reset_index())

demographics.to_csv("build/output/demographics_state_year.csv", index=False)</code></pre>
  </div>
</section>

<!-- ============================================================ -->
<!-- PART C: MERGING -->
<!-- ============================================================ -->
<section id="merging" class="tutorial-section">
  <h2>Part C: Merging Multiple Datasets</h2>

  <p>Now we merge four datasets together: (1) aggregated crashes, (2) state demographics (from Part C), (3) policy adoption dates, and (4) state names. Each merge uses <code>state_fips</code> as the key, with some using <code>year</code> as well.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* Convert CSVs to .dta for merging
import delimited "$build/input/policy_adoptions.csv", clear
save "$build/output/policy_adoptions.dta", replace

import delimited "$build/input/state_names.csv", clear
save "$build/output/state_names.dta", replace

* Load the main dataset and merge everything sequentially
use "$build/output/crashes_state_year.dta", clear

* Merge 1: Demographics — keep only matched rows
merge 1:1 state_fips year using "$build/output/demographics_state_year.dta", ///
    keep(match) nogen

* Merge 2: Policy adoptions — keep master + matched (some states never adopt)
merge m:1 state_fips using "$build/output/policy_adoptions.dta", ///
    keep(master match) nogen

* Merge 3: State names — keep only matched rows
merge m:1 state_fips using "$build/output/state_names.dta", ///
    keep(match) nogen

* Create treatment indicator
gen post_treated = (year >= adoption_year & !missing(adoption_year))
gen log_pop = ln(population)

* Save final analysis dataset
save "$build/output/analysis_panel.dta", replace
describe</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(data.table)

# Load all datasets
crashes_sy <- fread("build/output/crashes_state_year.csv")
demographics <- fread("build/output/demographics_state_year.csv")
policies <- fread("build/input/policy_adoptions.csv")
state_names <- fread("build/input/state_names.csv")

# Merge 1: Crashes + Demographics (on state_fips + year)
panel <- merge(crashes_sy, demographics, by = c("state_fips", "year"), all = FALSE)
cat("After merge 1:", nrow(panel), "rows\n")

# Merge 2: + Policy adoptions (on state_fips only — many:1)
panel <- merge(panel, policies, by = "state_fips", all.x = TRUE)
cat("After merge 2:", nrow(panel), "rows\n")

# Merge 3: + State names (on state_fips only — many:1)
panel <- merge(panel, state_names, by = "state_fips", all.x = TRUE)
cat("After merge 3:", nrow(panel), "rows\n")

# Create treatment indicator
panel[, post_treated := fifelse(
  !is.na(adoption_year) & year >= adoption_year, 1L, 0L
)]
panel[, log_pop := log(population)]

# Save final analysis dataset
fwrite(panel, "build/output/analysis_panel.csv")
str(panel)</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd
import numpy as np

# Load all datasets
crashes_sy = pd.read_csv("build/output/crashes_state_year.csv")
demographics = pd.read_csv("build/output/demographics_state_year.csv")
policies = pd.read_csv("build/input/policy_adoptions.csv")
state_names = pd.read_csv("build/input/state_names.csv")

# Merge 1: Crashes + Demographics (on state_fips + year)
panel = crashes_sy.merge(demographics, on=['state_fips', 'year'], how='inner')
print(f"After merge 1: {len(panel)} rows")

# Merge 2: + Policy adoptions (on state_fips only — many:1)
panel = panel.merge(policies, on='state_fips', how='left')
print(f"After merge 2: {len(panel)} rows")

# Merge 3: + State names (on state_fips only — many:1)
panel = panel.merge(state_names, on='state_fips', how='left')
print(f"After merge 3: {len(panel)} rows")

# Create treatment indicator
panel['post_treated'] = (
    (panel['year'] >= panel['adoption_year']) &
    panel['adoption_year'].notna()
).astype(int)
panel['log_pop'] = np.log(panel['population'])

# Save final analysis dataset
panel.to_csv("build/output/analysis_panel.csv", index=False)
print(panel.info())</code></pre>
  </div>

  <h3>Understanding Merge Types</h3>

  <p>Merges can be deceptively complicated. There are three types:</p>

  <ul>
    <li><strong>1:1</strong> — Each row in one dataset matches exactly one row in the other. This is the most common merge. Example: merging two state-year datasets on <code>state_fips</code> and <code>year</code>.</li>
    <li><strong>m:1</strong> (many-to-one) — Multiple rows in the master dataset match one row in the using dataset. Example: merging individual-level data with state-level variables. Many individuals share the same state, so each state-level row gets matched to multiple individual rows.</li>
    <li><strong>1:m</strong> (one-to-many) — The reverse: one row in the master matches many rows in the using dataset.</li>
  </ul>

  <p>In the code above, Merge 1 is a <strong>1:1</strong> merge (one row per state-year in both datasets). Merges 2 and 3 are <strong>m:1</strong> — the panel has many state-year rows per state, but the policy and state-name files have one row per state.</p>

  <h3>The <code>keep()</code> Option in Stata</h3>

  <p>In Stata, your original dataset is called "master" and the dataset you're merging in is called "using." After a merge, each observation is one of three types:</p>
  <ul>
    <li><strong>master</strong> — rows from the master that didn't match anything in the using data</li>
    <li><strong>match</strong> — rows that matched between master and using</li>
    <li><strong>using</strong> — rows from the using data that didn't match anything in the master</li>
  </ul>
  <p>You choose which to keep with the <code>keep()</code> option. Be careful — the wrong choice silently drops observations.</p>

  <p><strong>Example:</strong> Suppose you have a balanced state-year panel and want to merge in earthquake counts. You collapse the earthquake data to state-year totals, but states with <em>no earthquakes</em> don't appear in that dataset at all.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* BAD: keep(match) drops states with zero earthquakes!
merge 1:1 state year using "earthquakes.dta", keep(match)

* GOOD: keep(master match) preserves all original observations
merge 1:1 state year using "earthquakes.dta", keep(master match) nogen
replace num_earthquakes = 0 if missing(num_earthquakes)</code></pre>
    <pre class="code-content r"><code class="language-r"># BAD: inner join drops states with zero earthquakes
panel <- merge(panel, earthquakes, by = c("state", "year"))

# GOOD: left join preserves all original observations
panel <- merge(panel, earthquakes, by = c("state", "year"), all.x = TRUE)
panel[is.na(num_earthquakes), num_earthquakes := 0]</code></pre>
    <pre class="code-content python"><code class="language-python"># BAD: inner join drops states with zero earthquakes
panel = panel.merge(earthquakes, on=['state', 'year'])

# GOOD: left join preserves all original observations
panel = panel.merge(earthquakes, on=['state', 'year'], how='left')
panel['num_earthquakes'] = panel['num_earthquakes'].fillna(0)</code></pre>
  </div>

  <div class="callout-warning">
    <h4>The Rule of Thumb</h4>
    <p>Start with a dataset that contains all the observations you want. Use <code>keep(master match)</code> in Stata (or <code>how='left'</code> in R/Python) almost always. Then replace missing values with the appropriate value (often 0, meaning "no events") for any variable you merged in. This ensures your panel stays balanced and you never silently drop observations.</p>
  </div>

  <div class="callout-info">
    <h4>Always Check Your Merges</h4>
    <p>After every merge, verify: (1) the number of rows is what you expect, (2) there are no unexpected missing values, and (3) the merge type (1:1, m:1, 1:m) matches your data structure. In Stata, the <code>nogen</code> option suppresses the <code>_merge</code> variable. In R/Python, compare row counts before and after.</p>
  </div>
</section>

<!-- ============================================================ -->
<!-- PART D: DESCRIPTIVE STATISTICS TABLE -->
<!-- ============================================================ -->
<section id="descriptive-table" class="tutorial-section">
  <h2>Part D: Descriptive Statistics Table</h2>

  <p>Before running regressions, present a descriptive statistics table. A standard format in DD papers has four columns: (1) all observations, (2) treated states in the post-treatment period, (3) treated states before treatment, and (4) never-treated states. This shows how the groups compare and whether there are large pre-treatment differences.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/analysis_panel.dta", clear

* Create group indicator
gen group = 3 if missing(adoption_year)
replace group = 1 if !missing(adoption_year) & year >= adoption_year
replace group = 2 if !missing(adoption_year) & year < adoption_year

label define grp 1 "Treated After" 2 "Treated Before" 3 "Untreated"
label values group grp

* Encode region for factor variable (dtable requires numeric)
encode region, gen(census_region)
label variable census_region "Census Region"

* Label variables
label variable fatal_crashes "Fatal Crashes"
label variable serious_crashes "Serious Crashes"
label variable total_crashes "Total Crashes"
label variable fatal_share "Fatal Share"
label variable population "Population"
label variable median_income "Median Income"
label variable pct_urban "Pct. Urban"

* dtable produces "mean (sd)" format by default
dtable fatal_crashes serious_crashes total_crashes ///
    fatal_share population median_income pct_urban ///
    i.census_region, ///
    by(group) ///
    nformat(%14.2fc mean sd) ///
    sample(, statistics(freq) place(seplabels))

* Rename "Total" column to "All"
collect label levels group .m "All", modify
collect style header group, title(hide)

* Export LaTeX fragment
collect export "$analysis/output/tables/descriptive_table_raw.tex", tableonly replace

* Strip \begin{table} wrapper (dtable adds it; we just need the tabular)
filefilter "$analysis/output/tables/descriptive_table_raw.tex" ///
    "$analysis/output/tables/descriptive_table_s1.tex", ///
    from("\BSbegin{table}[!h]") to("") replace
filefilter "$analysis/output/tables/descriptive_table_s1.tex" ///
    "$analysis/output/tables/descriptive_table_s2.tex", ///
    from("\BScentering") to("") replace
filefilter "$analysis/output/tables/descriptive_table_s2.tex" ///
    "$analysis/output/tables/descriptive_table.tex", ///
    from("\BSend{table}") to("") replace</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(data.table, modelsummary)

panel <- fread("build/output/analysis_panel.csv")

# Create group column
panel[, ever_treated := !is.na(adoption_year)]
panel[, group := fifelse(
  !ever_treated, "Untreated",
  fifelse(post_treated == 1, "Treated x Post", "Treated x Pre")
)]

# Variables for the table
vars <- c("fatal_crashes", "serious_crashes", "total_crashes",
          "fatal_share", "population", "median_income", "pct_urban")

# Create publication-quality table with datasummary
datasummary(
  fatal_crashes + serious_crashes + total_crashes +
    fatal_share + population + median_income + pct_urban ~
    group * (Mean + SD),
  data = panel,
  fmt = 2,
  output = "analysis/output/tables/descriptive_table.tex"
)</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd

panel = pd.read_csv("build/output/analysis_panel.csv")

# Create group column
panel['group'] = 'Untreated'
panel.loc[panel['adoption_year'].notna() & (panel['post_treated'] == 1),
          'group'] = 'Treated x Post'
panel.loc[panel['adoption_year'].notna() & (panel['post_treated'] == 0),
          'group'] = 'Treated x Pre'

# Variables for the table
vars = ['fatal_crashes', 'serious_crashes', 'total_crashes',
        'fatal_share', 'population', 'median_income', 'pct_urban']

# Compute means and SDs by group
stats = (panel.groupby('group')[vars]
         .agg(['mean', 'std'])
         .round(2))

# Export to LaTeX
stats.T.to_latex("analysis/output/tables/descriptive_table.tex",
                 caption="Descriptive Statistics",
                 label="tab:desc")</code></pre>
  </div>

  <p>Here's what the exported table looks like when compiled to PDF:</p>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/descriptive_table.png" alt="Descriptive statistics table">
  </figure>

  <div class="callout-info">
    <h4>What to Look For</h4>
    <ul>
      <li><strong>Pre-treatment balance:</strong> Treated Before vs. Untreated should look similar. Big differences suggest selection into treatment.</li>
      <li><strong>Post-treatment changes:</strong> Compare Treated After vs. Treated Before. This is the raw before/after for treated units.</li>
      <li><strong>Sample composition:</strong> Report observation counts for each group at the bottom of the table.</li>
    </ul>
  </div>

  <h3>From Code Output to Publication-Ready Table</h3>

  <p>The code above produces a <code>.tex</code> file containing the table body. To get a publication-ready PDF, wrap it in a standalone LaTeX document and compile:</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">LaTeX wrapper</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">\documentclass[border=10pt]{standalone}
\usepackage{booktabs}
\begin{document}
\input{descriptive_table.tex}
\end{document}</code></pre>
  </div>

  <p>Compile with <code>pdflatex standalone_table.tex</code>. See the <a href="auxiliary-latex.html">LaTeX module</a> for installation details.</p>

  <div class="callout-warning">
    <h4>Table Formatting Checklist</h4>
    <ul>
      <li><strong>Use booktabs rules</strong> (<code>\toprule</code>, <code>\midrule</code>, <code>\bottomrule</code>) — never vertical lines or heavy grids</li>
      <li><strong>Standard errors in parentheses</strong> directly below coefficients</li>
      <li><strong>Stars for significance</strong> with a clear legend at the bottom</li>
      <li><strong>Round consistently:</strong> 2-3 decimal places for coefficients, 3 for standard errors</li>
      <li><strong>Label variables clearly:</strong> "Log(Population)" not "log_pop"</li>
      <li><strong>Fixed effects indicators:</strong> "Yes" or blank (not "No")</li>
      <li><strong>Sample size</strong> and <strong>R-squared</strong> at the bottom of every regression table</li>
    </ul>
  </div>

  <div class="callout-success">
    <h4>AI Tools Are Great for Table Formatting</h4>
    <p>Formatting tables and figures is an excellent use case for AI tools like ChatGPT or Claude. These tasks leave little room for hallucination—the AI just needs to adjust spacing, labels, and LaTeX formatting. Describe what you want and iterate.</p>
  </div>
</section>

<!-- ============================================================ -->
<!-- PART E: DIFF-IN-DIFF AND EVENT STUDY -->
<!-- ============================================================ -->
<section id="dd-analysis" class="tutorial-section">
  <h2>Part E: Difference-in-Differences and Event Study</h2>

  <h3>The TWFE Regression</h3>
  <p>With our merged panel, we estimate the effect of the policy on fatal crashes using two-way fixed effects (state + year FE).</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/analysis_panel.dta", clear

* Create log outcome
gen log_fatal = log(fatal_crashes + 1)

* Label variables for table output
label variable post_treated "Post × Treated"
label variable log_pop "Log Population"
label variable median_income "Median Income"

* Main DD regression: effect of policy on log fatal crashes
reghdfe log_fatal post_treated, absorb(state_fips year) vce(cluster state_fips)</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(fixest, data.table)

panel <- fread("build/output/analysis_panel.csv")
panel[, log_fatal := log(fatal_crashes + 1)]

# Main DD regression
main <- feols(
  log_fatal ~ post_treated | state_fips + year,
  data = panel, vcov = ~state_fips
)
summary(main)</code></pre>
    <pre class="code-content python"><code class="language-python">import pyfixest as pf
import pandas as pd
import numpy as np

panel = pd.read_csv("build/output/analysis_panel.csv")
panel['log_fatal'] = np.log(panel['fatal_crashes'] + 1)

# Main DD regression
main = pf.feols(
    "log_fatal ~ post_treated | state_fips + year",
    vcov={"CRV1": "state_fips"}, data=panel
)
main.summary()</code></pre>
  </div>

  <h3>Event Study</h3>
  <p>The event study shows dynamic treatment effects and lets us check the parallel trends assumption.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/analysis_panel.dta", clear

* Create time-to-treatment (never-treated get a far-away value)
gen time_to_treat = year - adoption_year
replace time_to_treat = -99 if missing(adoption_year)

* Create dummies: rel_m5 ... rel_m1, rel_0 ... rel_5
forvalues t = -5/5 {
    if `t' < 0 {
        local name "m`= abs(`t')'"
    }
    else {
        local name "`t'"
    }
    gen rel_`name' = (time_to_treat == `t')
}

* Bin endpoints
replace rel_m5 = (time_to_treat <= -5) & !missing(adoption_year)
replace rel_5  = (time_to_treat >= 5)  & !missing(adoption_year)

* Create log outcome
gen log_fatal = log(fatal_crashes + 1)

* Regression (omit t=-1)
reghdfe log_fatal rel_m5 rel_m4 rel_m3 rel_m2 ///
    rel_0 rel_1 rel_2 rel_3 rel_4 rel_5 log_pop, ///
    absorb(state_fips year) vce(cluster state_fips)

estimates store event_study

* Extract coefficients and SEs into matrices
matrix b = e(b)
matrix V = e(V)

preserve
clear
set obs 11
gen t = .
gen coef = .
gen se = .

local coefs "rel_m5 rel_m4 rel_m3 rel_m2 rel_0 rel_1 rel_2 rel_3 rel_4 rel_5"
local i = 1
foreach v of local coefs {
    if `i' <= 4 { replace t = `i' - 6 in `i' }
    else        { replace t = `i' - 5 in `i' }
    replace coef = b[1, `i'] in `i'
    replace se = sqrt(V[`i', `i']) in `i'
    local i = `i' + 1
}

* Add omitted period (t = -1, coefficient = 0)
replace t = -1 in 11
replace coef = 0 in 11
replace se = 0 in 11

gen ub = coef + 1.96 * se
gen lb = coef - 1.96 * se
sort t

* Shaded confidence bands (ribbons) + connected points
twoway (rarea ub lb t, color("44 95 138%20") lwidth(none)) ///
       (connected coef t, mcolor("44 95 138") lcolor("44 95 138") ///
            msize(small) msymbol(O) lwidth(medthin)), ///
    yline(0, lcolor(gs8) lwidth(thin)) ///
    xline(-0.5, lcolor(gs8) lwidth(thin) lpattern(dash)) ///
    xtitle("Years Relative to Policy Adoption", ///
           size(large) margin(t=3) color(gs5)) ///
    ytitle("") ///
    ylabel(, labsize(large) labcolor(gs5) grid glcolor(gs14) glwidth(vthin)) ///
    xlabel(-5(1)5, labsize(large) labcolor(gs5)) ///
    title("Effect on Log Fatal Crashes", ///
          position(11) size(large) color(gs5)) ///
    legend(off) ///
    graphregion(color(white) margin(medium)) ///
    plotregion(color(white) margin(small))

graph export "$analysis/output/figures/event_study.png", replace width(2400) height(1800)
restore</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(fixest, ggfixest, data.table)

panel <- fread("build/output/analysis_panel.csv")
panel[, log_fatal := log(fatal_crashes + 1)]

# Create time-to-treatment
# Never-treated: set to -1000
panel[, time_to_treat := fifelse(
  is.na(adoption_year), -1000L,
  as.integer(year - adoption_year)
)]
panel[, ever_treated := fifelse(!is.na(adoption_year), 1L, 0L)]

# Bin endpoints
panel[, time_to_treat := fifelse(
  time_to_treat == -1000, -1000L,
  as.integer(pmax(-5, pmin(5, time_to_treat)))
)]

# Event study regression
es <- feols(
  log_fatal ~ i(time_to_treat, ever_treated,
                     ref = c(-1, -1000)) + log_pop
    | state_fips + year,
  data = panel, vcov = ~state_fips
)
summary(es)

# Plot
ggiplot(es) +
  labs(x = "Years Relative to Policy Adoption",
       y = "Effect on Log Fatal Crashes") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  theme_minimal()
ggsave("analysis/output/figures/event_study.png", width = 8, height = 5, dpi = 300)</code></pre>
    <pre class="code-content python"><code class="language-python">import pyfixest as pf
import pandas as pd
import numpy as np

panel = pd.read_csv("build/output/analysis_panel.csv")
panel['log_fatal'] = np.log(panel['fatal_crashes'] + 1)

# Create time-to-treatment
panel['time_to_treat'] = panel['year'] - panel['adoption_year']
panel['time_to_treat'] = panel['time_to_treat'].fillna(-1000).astype(int)
panel['ever_treated'] = panel['adoption_year'].notna().astype(int)

# Bin endpoints
panel['time_to_treat'] = np.where(
    panel['time_to_treat'] == -1000, -1000,
    np.clip(panel['time_to_treat'], -5, 5)
)

# Event study regression
es = pf.feols(
    "log_fatal ~ i(time_to_treat, ever_treated, ref=c(-1, -1000))"
    " + log_pop | state_fips + year",
    vcov={"CRV1": "state_fips"}, data=panel
)
es.summary()

# Plot
es.iplot()</code></pre>
  </div>

  <p>Here's the event study plot produced by the Stata code above:</p>
  <figure class="output-figure narrow">
    <img src="data/project-walkthrough/figures/event_study.png" alt="Event study plot">
    <figcaption>Event study showing dynamic treatment effects. Pre-treatment coefficients near zero support the parallel trends assumption.</figcaption>
  </figure>

  <h3>Making Figures Publication-Ready</h3>

  <p>Stata's default graphs look terrible. With a few tweaks, you can produce figures that look like they belong in a top journal. The key reference is <strong>Schwabish (2014)</strong>, <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.28.1.209" target="_blank">"An Economist's Guide to Visualizing Data"</a> (<em>JEP</em>). Tal Gross has an excellent thread showing exactly how to transform Stata's defaults into clean, publication-quality output:</p>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfmixrc23" data-bluesky-cid="bafyreihlukkgjwe52lynfbe7omkryyu7zzuqntc3ajhihingmgcl7vlhxe"><p lang="en">Make your graphs better in three easy steps…</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfmixrc23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfmj6m223" data-bluesky-cid="bafyreidjlo6y6jrr55p2loflddfepwwu2qm32xsvi53frozzvjljg3udee"><p lang="en">To start, here's a graph with the default Stata options.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfmj6m223?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfn3bpc23" data-bluesky-cid="bafyreih7dwnh4djhrbhpci7rlseybjok4qbi7u52gjtwv6pzcrr46bs5au"><p lang="en">Step 1: labels not legends. This makes the graph much easier to read.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfn3bpc23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfnklws23" data-bluesky-cid="bafyreicxbhyxwiaxxkkbbxypwqarqrwve7u4sdcsuryujmtx6y7dhli7qi"><p lang="en">Step 2: all text horizontal. Horizontal text is easier to read than vertical text.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfnklws23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfo2rjc23" data-bluesky-cid="bafyreic7ihkvkmveyjt6gfk7wabush7vl7k5dym3z6emltvowaepmgzfgi"><p lang="en">Step 3: eliminated unnecessary ink.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfo2rjc23?ref_src=embed">March 11, 2025</a></blockquote>

  <blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:bubn3uwhrtxpro2cruxfdsee/app.bsky.feed.post/3lk4bfoqqnc23" data-bluesky-cid="bafyreidkwcxj6x7gvzeeyrgtlnxmomeka25dtyqxb47xorzmqkuyhfvqbm"><p lang="en">That's Stata, here's a start in R.</p>&mdash; <a href="https://bsky.app/profile/talgross.bsky.social?ref_src=embed">Tal Gross (@talgross.bsky.social)</a> <a href="https://bsky.app/profile/talgross.bsky.social/post/3lk4bfoqqnc23?ref_src=embed">March 11, 2025</a></blockquote>

  <script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>

  <p>The event study plot above follows these principles. Here's a summary of the key steps:</p>

  <ol>
    <li><strong>Remove clutter.</strong> No top/right borders (<code>graphregion(color(white))</code>), no unnecessary gridlines, no legend when you can label directly.</li>
    <li><strong>Use a single accent color</strong> for your main result. Use gray for reference lines and context. The code above uses <code>mcolor("44 95 138")</code> — a muted blue.</li>
    <li><strong>Shaded confidence bands are cleaner than error bars</strong> for continuous data. In Stata, use <code>rarea</code> for shaded bands instead of <code>rcap</code> for error bars. The event study code above demonstrates this — the <code>twoway rarea</code> command creates the shaded CI bands, while <code>connected</code> plots the point estimates.</li>
    <li><strong>Informative axis labels.</strong> "Years Relative to Policy Adoption" not "period". Use <code>xtitle()</code> and <code>ytitle()</code> with readable sizes.</li>
    <li><strong>Remove the plot title.</strong> Economics papers put the title in the figure caption, not on the plot itself.</li>
    <li><strong>Export at high resolution.</strong> Use <code>graph export "file.png", width(2400)</code> or export as PDF for vector graphics.</li>
  </ol>

  <div class="callout-info">
    <h4>Figure Checklist</h4>
    <ul>
      <li>Can you read all text when the figure is printed at its final size?</li>
      <li>Are axis labels descriptive? (not variable names)</li>
      <li>Is the aspect ratio wider than tall? (roughly 4:3 or 16:10)</li>
      <li>Is the resolution at least 300 DPI for print?</li>
      <li>Did you export as PDF (vector) or high-res PNG?</li>
      <li>Is the background white? (no gray default)</li>
      <li>Are confidence intervals shown as shaded bands (not error bars) where appropriate?</li>
    </ul>
  </div>
</section>

<!-- ============================================================ -->
<!-- PART F: PROFESSIONAL DD TABLE WITH SUBGROUPS -->
<!-- ============================================================ -->
<section id="dd-table" class="tutorial-section">
  <h2>Part F: Professional DD Table with Subgroups</h2>

  <p>A strong empirical paper doesn't just report the main result. It shows robustness across specifications and explores heterogeneity. Here we create a single table with: (1) the main result, (2) results by region subgroup, and (3) a specification with additional controls.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">use "$build/output/analysis_panel.dta", clear

* Label variables for table output
label variable post_treated "Post × Treated"
label variable log_pop "Log Population"
label variable median_income "Median Income"

* Col 1: Main result
reghdfe fatal_crashes post_treated, absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m1

* Col 2: With controls
reghdfe fatal_crashes post_treated log_pop median_income, ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m2

* Col 3: South only
reghdfe fatal_crashes post_treated log_pop if region == "South", ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m3

* Col 4: Non-South
reghdfe fatal_crashes post_treated log_pop if region != "South", ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m4

* Col 5: Serious crashes as outcome
reghdfe serious_crashes post_treated log_pop, ///
    absorb(state_fips year) vce(cluster state_fips)
estadd local state_fe "Yes"
estadd local year_fe "Yes"
estimates store m5

* Export professional table
esttab m1 m2 m3 m4 m5 ///
    using "$analysis/output/tables/dd_results.tex", replace ///
    se(%9.3f) b(%9.3f) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    nomtitles nonumbers ///
    label fragment ///
    prehead("\begin{tabular}{l*{5}{c}}" ///
            "\midrule \midrule" ///
            "Dependent Variables: &\multicolumn{4}{c}{Fatal Crashes} &Serious\\" ///
            "\cmidrule(lr){2-5} \cmidrule(lr){6-6}" ///
            "Sample: & All & All & South & Non-South & All \\" ///
            "Model: & (1) & (2) & (3) & (4) & (5) \\" ///
            "\midrule" ///
            "\emph{Variables} \\") ///
    posthead("") ///
    prefoot("\midrule" ///
            "\emph{Fixed-effects} \\") ///
    stats(state_fe year_fe N r2, ///
        labels("State FE" "Year FE" ///
               "\midrule \emph{Fit statistics} \\ Observations" ///
               "R\$^2\$") ///
        fmt(%s %s %9.0fc %9.3f)) ///
    postfoot("\midrule \midrule" ///
             "\multicolumn{6}{l}{\emph{Clustered (state) standard-errors in parentheses}}\\" ///
             "\multicolumn{6}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\" ///
             "\end{tabular}")</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(fixest, data.table)

panel <- fread("build/output/analysis_panel.csv")

# Col 1: Main result
m1 <- feols(fatal_crashes ~ post_treated | state_fips + year,
            data = panel, vcov = ~state_fips)

# Col 2: With controls
m2 <- feols(fatal_crashes ~ post_treated + log_pop + median_income
            | state_fips + year,
            data = panel, vcov = ~state_fips)

# Col 3: South only
m3 <- feols(fatal_crashes ~ post_treated + log_pop | state_fips + year,
            data = panel[region == "South"], vcov = ~state_fips)

# Col 4: Non-South
m4 <- feols(fatal_crashes ~ post_treated + log_pop | state_fips + year,
            data = panel[region != "South"], vcov = ~state_fips)

# Col 5: Serious crashes as outcome
m5 <- feols(serious_crashes ~ post_treated + log_pop | state_fips + year,
            data = panel, vcov = ~state_fips)

# Export professional table
etable(
  m1, m2, m3, m4, m5,
  tex = TRUE, file = "analysis/output/tables/dd_table.tex", replace = TRUE,
  headers = list(
    ":_sym:" = c("(1)", "(2)", "(3)", "(4)", "(5)")
  ),
  style.tex = style.tex(
    depvar.title = "Dep. Var.:",
    fixef.title = "\\midrule",
    yesNo = c("Yes", ""),
    stats.title = "\\midrule"
  ),
  fitstat = ~ n + r2 + wr2,
  dict = c(
    fatal_crashes = "Fatal Crashes",
    serious_crashes = "Serious Crashes",
    post_treated = "Post × Treated",
    log_pop = "Log(Population)",
    median_income = "Median Income"
  ),
  se.below = TRUE, depvar = TRUE
)</code></pre>
    <pre class="code-content python"><code class="language-python">import pyfixest as pf
import pandas as pd

panel = pd.read_csv("build/output/analysis_panel.csv")

# Col 1: Main result
m1 = pf.feols("fatal_crashes ~ post_treated | state_fips + year",
              vcov={"CRV1": "state_fips"}, data=panel)

# Col 2: With controls
m2 = pf.feols("fatal_crashes ~ post_treated + log_pop + median_income"
              " | state_fips + year",
              vcov={"CRV1": "state_fips"}, data=panel)

# Col 3: South only
m3 = pf.feols("fatal_crashes ~ post_treated + log_pop | state_fips + year",
              vcov={"CRV1": "state_fips"},
              data=panel[panel['region'] == 'South'])

# Col 4: Non-South
m4 = pf.feols("fatal_crashes ~ post_treated + log_pop | state_fips + year",
              vcov={"CRV1": "state_fips"},
              data=panel[panel['region'] != 'South'])

# Col 5: Serious crashes as outcome
m5 = pf.feols("serious_crashes ~ post_treated + log_pop | state_fips + year",
              vcov={"CRV1": "state_fips"}, data=panel)

# Export table
pf.etable(
    [m1, m2, m3, m4, m5],
    type="tex",
    file="analysis/output/tables/dd_table.tex",
    labels={
        "fatal_crashes": "Fatal Crashes",
        "serious_crashes": "Serious Crashes",
        "post_treated": "Post × Treated",
        "log_pop": "Log(Population)",
        "median_income": "Median Income",
    },
)</code></pre>
  </div>

  <p>Here's the DD regression table produced by <code>esttab</code>:</p>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/dd_results.png" alt="DD regression table">
  </figure>
</section>

<!-- ============================================================ -->
<!-- PART G: INSTRUMENTAL VARIABLES -->
<!-- ============================================================ -->
<section id="iv-analysis" class="tutorial-section">
  <h2>Part G: Instrumental Variables</h2>

  <p>This section uses a separate, pre-built dataset (<code>iv_data.csv</code>) inspired by Angrist &amp; Lavy (1999). We estimate the effect of class size on test scores, using a Maimonides-rule instrument (enrollment determines class size via a maximum-of-40 rule).</p>

  <h3>The Setup</h3>
  <ul>
    <li><strong>Endogenous variable:</strong> class_size (correlated with school quality)</li>
    <li><strong>Instrument:</strong> enrollment (determines class size via the Maimonides rule, but plausibly doesn't directly affect test scores)</li>
    <li><strong>Outcome:</strong> test_score</li>
  </ul>

  <h3>First Stage</h3>
  <p>Does the instrument predict the endogenous variable? Check the <strong>F-statistic</strong> — it should be well above 10.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">import delimited "$analysis/code/iv_data.csv", clear

* Label variables
label variable class_size "Class Size"
label variable test_score "Test Score"
label variable enrollment "Enrollment"
label variable pct_disadvantaged "Pct. Disadvantaged"

* First stage: Does enrollment predict class size?
regress class_size enrollment pct_disadvantaged, vce(robust)
estimates store first_stage

* Check the F-stat on enrollment — should be >> 10
test enrollment</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(fixest, data.table)

iv_data <- fread("analysis/code/iv_data.csv")

# First stage: Does enrollment predict class size?
first_stage <- feols(class_size ~ enrollment + pct_disadvantaged,
                     data = iv_data)
summary(first_stage)

# F-statistic on excluded instrument
cat("First-stage F:", fitstat(first_stage, "f")$f$stat, "\n")</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd
import pyfixest as pf

iv_data = pd.read_csv("analysis/code/iv_data.csv")

# First stage: Does enrollment predict class size?
first_stage = pf.feols(
    "class_size ~ enrollment + pct_disadvantaged",
    data=iv_data, vcov="hetero"
)
first_stage.summary()

# F-statistic on excluded instrument
enrollment_coef = first_stage.coef()['enrollment']
enrollment_se = first_stage.se()['enrollment']
f_stat = (enrollment_coef / enrollment_se) ** 2
print(f"\nFirst-stage F-statistic: {f_stat:.1f}")</code></pre>
  </div>

  <h3>Reduced Form</h3>
  <p>The reduced form regresses the outcome directly on the instrument. If the instrument affects the outcome, it's working through the endogenous variable.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* Reduced form: Does enrollment predict test scores?
regress test_score enrollment pct_disadvantaged, vce(robust)
estimates store reduced_form</code></pre>
    <pre class="code-content r"><code class="language-r"># Reduced form: Does enrollment predict test scores?
reduced_form <- feols(test_score ~ enrollment + pct_disadvantaged,
                      data = iv_data)
summary(reduced_form)</code></pre>
    <pre class="code-content python"><code class="language-python"># Reduced form: Does enrollment predict test scores?
reduced_form = pf.feols(
    "test_score ~ enrollment + pct_disadvantaged",
    data=iv_data, vcov="hetero"
)
reduced_form.summary()</code></pre>
  </div>

  <h3>2SLS Estimation</h3>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* OLS (biased estimate)
regress test_score class_size pct_disadvantaged, vce(robust)
estimates store ols

* 2SLS: Instrument class_size with enrollment
ivregress 2sls test_score pct_disadvantaged (class_size = enrollment), ///
    vce(robust) first
estimates store iv_2sls

* Export all four specifications
esttab first_stage reduced_form iv_2sls ols ///
    using "$analysis/output/tables/iv_results.tex", replace ///
    label fragment ///
    order(class_size enrollment pct_disadvantaged) ///
    b(%9.3f) se(%9.3f) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    nomtitles nonumbers ///
    prehead("\begin{tabular}{l*{4}{c}}" ///
            "\midrule \midrule" ///
            "Dependent Variable: & Class Size & \multicolumn{3}{c}{Test Score}\\" ///
            "\cmidrule(lr){2-2} \cmidrule(lr){3-5}" ///
            "Model: & First Stage & Reduced Form & 2SLS & OLS\\" ///
            "& (1) & (2) & (3) & (4)\\" ///
            "\midrule" ///
            "\emph{Variables} \\") ///
    posthead("") ///
    prefoot("\midrule" ///
            "\emph{Fit statistics} \\") ///
    stats(N r2, fmt(%9.0fc %9.3f) ///
          labels("Observations" "R\$^2\$")) ///
    postfoot("\midrule \midrule" ///
             "\multicolumn{5}{l}{\emph{Heteroskedasticity-robust standard-errors in parentheses}}\\" ///
             "\multicolumn{5}{l}{\emph{Instrument: School enrollment}}\\" ///
             "\multicolumn{5}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\" ///
             "\end{tabular}")</code></pre>
    <pre class="code-content r"><code class="language-r"># 2SLS: Instrument class_size with enrollment
# fixest syntax: outcome ~ controls | FE | endogenous ~ instrument
iv_model <- feols(
  test_score ~ pct_disadvantaged | 0 | class_size ~ enrollment,
  data = iv_data
)
summary(iv_model)

# First-stage F
cat("First-stage F:", fitstat(iv_model, "ivf")$ivf$stat, "\n")

# Compare OLS vs IV
ols_model <- feols(test_score ~ class_size + pct_disadvantaged,
                   data = iv_data)
etable(ols_model, iv_model,
       dict = c(class_size = "Class Size",
                fit_class_size = "Class Size (IV)"))</code></pre>
    <pre class="code-content python"><code class="language-python">import pyfixest as pf

# 2SLS: Instrument class_size with enrollment
iv_model = pf.feols(
    "test_score ~ pct_disadvantaged | 0 | class_size ~ enrollment",
    data=iv_data, vcov="hetero"
)
iv_model.summary()

# Compare: OLS
ols_model = pf.feols(
    "test_score ~ class_size + pct_disadvantaged",
    data=iv_data, vcov="hetero"
)

# Compare coefficients
print(f"\nOLS class_size coef: {ols_model.coef()['class_size']:.3f}")
print(f"IV  class_size coef: {iv_model.coef()['class_size']:.3f}")</code></pre>
  </div>

  <p>Here's the IV results table comparing OLS, first stage, reduced form, and 2SLS:</p>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/iv_results.png" alt="IV regression table">
  </figure>

  <div class="callout-warning">
    <h4>OLS vs. IV: What to Expect</h4>
    <p>OLS is biased because class size is correlated with school quality (omitted variable bias). The IV estimate isolates the causal effect by using only variation in class size driven by enrollment thresholds. The IV estimate should typically be larger in magnitude if OLS is biased toward zero (attenuation bias from measurement error) or could go either way depending on the source of endogeneity.</p>
  </div>
</section>

<!-- ============================================================ -->
<!-- PART H: REGRESSION DISCONTINUITY -->
<!-- ============================================================ -->
<section id="rd-analysis" class="tutorial-section">
  <h2>Part H: Regression Discontinuity</h2>

  <p>This section uses a pre-built dataset (<code>rd_data.csv</code>) inspired by Carpenter &amp; Dobkin (2009). We estimate the effect of legal drinking access (turning 21) on mortality using a sharp RD design.</p>

  <h3>The Setup</h3>
  <ul>
    <li><strong>Running variable:</strong> days_from_21 (days relative to 21st birthday)</li>
    <li><strong>Cutoff:</strong> 0 (turning 21)</li>
    <li><strong>Treatment:</strong> over_21 (legal access to alcohol)</li>
    <li><strong>Outcome:</strong> mortality_rate</li>
  </ul>

  <h3>Linear Specification</h3>
  <p>The simplest RD: a linear function of the running variable on each side of the cutoff, within a bandwidth.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">import delimited "$analysis/code/rd_data.csv", clear

* Label variables
label variable mortality_rate "Mortality Rate"
label variable over_21 "Over 21"
label variable days_from_21 "Days from 21st Birthday"

* Create interaction term
gen days_x_over21 = days_from_21 * over_21

* Linear RD within bandwidth of 365 days
regress mortality_rate over_21 days_from_21 days_x_over21 ///
    if abs(days_from_21) <= 365, vce(robust)
estimates store rd_linear

* The coefficient on over_21 is the RD estimate (jump at cutoff)</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(fixest, data.table)

rd_data <- fread("analysis/code/rd_data.csv")

# Linear RD within bandwidth of 365 days
rd_linear <- feols(
  mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21,
  data = rd_data[abs(days_from_21) <= 365]
)
summary(rd_linear)

# Narrower bandwidth (180 days)
rd_narrow <- feols(
  mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21,
  data = rd_data[abs(days_from_21) <= 180]
)
summary(rd_narrow)</code></pre>
    <pre class="code-content python"><code class="language-python">import pandas as pd
import pyfixest as pf

rd_data = pd.read_csv("analysis/code/rd_data.csv")

# Linear RD within bandwidth of 365 days
subset = rd_data[rd_data['days_from_21'].abs() <= 365].copy()
rd_linear = pf.feols(
    'mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21',
    data=subset, vcov="hetero"
)
print("=== Linear RD (bw=365) ===")
rd_linear.summary()

# Narrower bandwidth (180 days)
subset_narrow = rd_data[rd_data['days_from_21'].abs() <= 180].copy()
rd_narrow = pf.feols(
    'mortality_rate ~ over_21 + days_from_21 + over_21:days_from_21',
    data=subset_narrow, vcov="hetero"
)
print("\n=== Linear RD (bw=180) ===")
rd_narrow.summary()</code></pre>
  </div>

  <h3>Polynomial Specification</h3>
  <p>A polynomial specification allows for curvature in the relationship between the running variable and the outcome. This can be more flexible but risks overfitting.</p>

  <div class="code-block">
    <div class="code-header">
      <div class="code-tabs">
        <button class="code-tab active" data-lang="stata">Stata</button>
        <button class="code-tab" data-lang="r">R</button>
        <button class="code-tab" data-lang="python">Python</button>
      </div>
      <button class="copy-btn" onclick="copyCode(this)">Copy</button>
    </div>
    <pre class="code-content stata active"><code class="language-stata">* Create polynomial terms
gen days_sq = days_from_21^2
gen days_cu = days_from_21^3
gen days_sq_x_over21 = days_sq * over_21
gen days_cu_x_over21 = days_cu * over_21

* Quadratic RD
regress mortality_rate over_21 days_from_21 days_x_over21 ///
    days_sq days_sq_x_over21 ///
    if abs(days_from_21) <= 365, vce(robust)
estimates store rd_quadratic

* Cubic RD
regress mortality_rate over_21 days_from_21 days_x_over21 ///
    days_sq days_sq_x_over21 days_cu days_cu_x_over21 ///
    if abs(days_from_21) <= 365, vce(robust)
estimates store rd_cubic

* Export results table
esttab rd_linear rd_quadratic rd_cubic ///
    using "$analysis/output/tables/rd_results.tex", replace ///
    keep(over_21) label fragment ///
    b(%9.4f) se(%9.4f) ///
    star(* 0.10 ** 0.05 *** 0.01) ///
    nomtitles nonumbers ///
    prehead("\begin{tabular}{l*{3}{c}}" ///
            "\midrule \midrule" ///
            "Dependent Variable: &\multicolumn{3}{c}{Mortality Rate}\\" ///
            "\cmidrule(lr){2-4}" ///
            "Model:              & Linear & Quadratic & Cubic\\" ///
            "                    & (1) & (2) & (3)\\" ///
            "\midrule" ///
            "\emph{Variables} \\") ///
    posthead("") ///
    prefoot("\midrule" ///
            "\emph{Fit statistics} \\") ///
    stats(N r2, fmt(%9.0fc %9.4f) ///
          labels("Observations" "R\$^2\$")) ///
    postfoot("\midrule \midrule" ///
             "\multicolumn{4}{l}{\emph{Robust standard-errors in parentheses}}\\" ///
             "\multicolumn{4}{l}{\emph{Signif. Codes: ***: 0.01, **: 0.05, *: 0.1}}\\" ///
             "\end{tabular}")</code></pre>
    <pre class="code-content r"><code class="language-r">pacman::p_load(fixest, rdrobust, data.table)

rd_data <- fread("analysis/code/rd_data.csv")

# Quadratic RD
rd_data[, days2 := days_from_21^2]
rd_quad <- feols(
  mortality_rate ~ over_21 + days_from_21 + days2 +
    over_21:days_from_21 + over_21:days2,
  data = rd_data[abs(days_from_21) <= 365]
)
summary(rd_quad)

# Using rdrobust for optimal bandwidth selection
rd_robust <- rdrobust(
  y = rd_data$mortality_rate,
  x = rd_data$days_from_21,
  c = 0
)
summary(rd_robust)

# RD plot
rdplot(
  y = rd_data$mortality_rate,
  x = rd_data$days_from_21,
  c = 0,
  title = "RD Plot: Mortality at Age 21",
  x.label = "Days from 21st Birthday",
  y.label = "Mortality Rate"
)</code></pre>
    <pre class="code-content python"><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import pyfixest as pf

rd_data = pd.read_csv("analysis/code/rd_data.csv")

# Quadratic RD
rd_data['days2'] = rd_data['days_from_21'] ** 2
subset = rd_data[rd_data['days_from_21'].abs() <= 365].copy()
rd_quad = pf.feols(
    'mortality_rate ~ over_21 + days_from_21 + days2'
    ' + over_21:days_from_21 + over_21:days2',
    data=subset, vcov="hetero"
)
print("=== Quadratic RD ===")
rd_quad.summary()

# RD Plot
fig, ax = plt.subplots(figsize=(8, 5))
# Bin the data
bins = np.arange(-365, 366, 30)
rd_data['bin'] = pd.cut(rd_data['days_from_21'], bins=bins)
binned = rd_data.groupby('bin')['mortality_rate'].mean().reset_index()
binned['mid'] = binned['bin'].apply(lambda x: x.mid if pd.notna(x) else np.nan)
binned = binned.dropna()

ax.scatter(binned['mid'], binned['mortality_rate'], s=20, alpha=0.7)
ax.axvline(0, color='red', linestyle='--', linewidth=0.8)
ax.set_xlabel('Days from 21st Birthday')
ax.set_ylabel('Mortality Rate')
ax.set_title('RD Plot: Mortality at Age 21')
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
fig.tight_layout()
fig.savefig('analysis/output/figures/rd_plot.png', dpi=300)</code></pre>
  </div>

  <p>Here's the RD plot and regression table:</p>
  <figure class="output-figure narrow">
    <img src="data/project-walkthrough/figures/rd_plot.png" alt="RD plot with linear fit">
    <figcaption>RD plot with linear fit showing a clear discontinuity in mortality at age 21.</figcaption>
  </figure>
  <figure class="output-figure narrow">
    <img src="data/project-walkthrough/figures/rd_plot_poly.png" alt="RD plot with polynomial fit">
    <figcaption>RD plot with polynomial fit. The discontinuity is robust to allowing curvature in the running variable.</figcaption>
  </figure>
  <figure class="output-figure">
    <img src="data/project-walkthrough/figures/rd_results.png" alt="RD regression table">
  </figure>

  <div class="callout-info">
    <h4>Linear vs. Polynomial: Which to Use?</h4>
    <p>Start with <strong>linear</strong> — it's simpler and less prone to overfitting. Use polynomial specifications as robustness checks. If the linear and polynomial estimates are very different, it suggests the result may be sensitive to functional form assumptions. The <code>rdrobust</code> package provides data-driven bandwidth selection and bias-corrected inference, and is the current best practice.</p>
  </div>
</section>
